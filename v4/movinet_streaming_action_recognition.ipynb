{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yx2wbEy63MI",
        "outputId": "02f40cd0-934f-4099-8e57-adcc8bcebcbc"
      },
      "source": [
        "https://www.tensorflow.org/hub/tutorials/movinet\n",
        "\n",
        "\n",
        "!sudo apt install -y ffmpeg\n",
        "\n",
        "!pip install -q mediapy\n",
        "\n",
        "!pip uninstall -q -y opencv-python-headless\n",
        "\n",
        "!pip install -q \"opencv-python-headless<4.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7_7s8VO486Dq"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pathlib\n",
        "import cv2\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tqdm\n",
        "\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 10,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWGHE_7D8-6u",
        "outputId": "3816bfe1-c5cb-4655-e2c3-05df41c6328c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['abseiling', 'acting in play', 'adjusting glasses', 'air drumming',\n",
              "       'alligator wrestling', 'answering questions', 'applauding',\n",
              "       'applying cream', 'archaeological excavation', 'archery',\n",
              "       'arguing', 'arm wrestling', 'arranging flowers',\n",
              "       'assembling bicycle', 'assembling computer',\n",
              "       'attending conference', 'auctioning', 'backflip (human)',\n",
              "       'baking cookies', 'bandaging'], dtype='<U49')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_path = tf.keras.utils.get_file(\n",
        "    fname='labels.txt',\n",
        "    origin='https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt'\n",
        ")\n",
        "labels_path = pathlib.Path(labels_path)\n",
        "\n",
        "lines = labels_path.read_text().splitlines()\n",
        "KINETICS_600_LABELS = np.array([line.strip() for line in lines])\n",
        "KINETICS_600_LABELS[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbmRYHi2-TPh",
        "outputId": "60821ecf-73e2-445c-9e06-b0ca709f9750"
      },
      "outputs": [],
      "source": [
        "jumpingjack_url = 'https://github.com/tensorflow/models/raw/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/jumpingjack.gif'\n",
        "jumpingjack_path = tf.keras.utils.get_file(\n",
        "    fname='jumpingjack.gif',\n",
        "    origin=jumpingjack_url,\n",
        "    cache_dir='.', cache_subdir='.',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EvEz3tun-x3g"
      },
      "outputs": [],
      "source": [
        "# Read and process a video\n",
        "def load_gif(file_path, image_size=(224, 224)):\n",
        "  \"\"\"Loads a gif file into a TF tensor.\n",
        "\n",
        "  Use images resized to match what's expected by your model.\n",
        "  The model pages say the \"A2\" models expect 224 x 224 images at 5 fps\n",
        "\n",
        "  Args:\n",
        "    file_path: path to the location of a gif file.\n",
        "    image_size: a tuple of target size.\n",
        "\n",
        "  Returns:\n",
        "    a video of the gif file\n",
        "  \"\"\"\n",
        "  # Load a gif file, convert it to a TF tensor\n",
        "  raw = tf.io.read_file(file_path)\n",
        "  video = tf.io.decode_gif(raw)\n",
        "  # Resize the video\n",
        "  video = tf.image.resize(video, image_size)\n",
        "  # change dtype to a float32\n",
        "  # Hub models always want images normalized to [0,1]\n",
        "  # ref: https://www.tensorflow.org/hub/common_signatures/images#input\n",
        "  video = tf.cast(video, tf.float32) / 255.\n",
        "  return video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[508, 947, 1201, 1578, 1959, 2492, 2881, 3136, 3377, 3727, 5149, 6310, 6848]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "with open('bite_frame_indexes.json') as f:\n",
        "    bite_frame_indexes = json.load(f)\n",
        "bite_frame_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n",
            "5500\n",
            "6000\n",
            "6500\n",
            "7000\n",
            "7500\n"
          ]
        }
      ],
      "source": [
        "from extract_face import extract_face\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    # Resize the frame to the input size expected by the MoViNet model\n",
        "    # frame = cv2.resize(frame, (224, 224))  # Adjust size if your model uses different dimensions\n",
        "    # # Normalize the frame (0-255 to 0-1)\n",
        "    try:\n",
        "        frame = extract_face(frame)\n",
        "        frame = frame / 255.0\n",
        "        # frame = frame[0:720, 280:1000]\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        return 1 - frame\n",
        "    except:\n",
        "        return np.zeros((224, 224, 3))\n",
        "\n",
        "def load_bite_clip():\n",
        "    cap = cv2.VideoCapture('raw_session.mp4')\n",
        "    frames = {}\n",
        "    frame_number = -1\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_number += 1\n",
        "        if frame_number % 500 == 0:\n",
        "            print(frame_number)\n",
        "        if frame_number % 6 != 0:  # Expects 5fps, source is 30, skip every 6\n",
        "            continue\n",
        "        # if frame_number < 1050 or frame_number % 6 != 0:\n",
        "        #     continue\n",
        "        # if frame_number > 1700:\n",
        "        #     break\n",
        "\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        processed_frame = preprocess_frame(frame)\n",
        "        frames[frame_number] = processed_frame\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return frames\n",
        "\n",
        "bite_clip_all_frames = load_bite_clip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bites: 13\n",
            "Non bites: 206\n",
            "Train bites: 10\n",
            "Train non bites: 164\n",
            "Test bites: 3\n",
            "Test non bites: 42\n"
          ]
        }
      ],
      "source": [
        "frames_per_clip = 6\n",
        "frame_step = 6\n",
        "\n",
        "bites = []\n",
        "non_bites = []\n",
        "current_clip = []\n",
        "for i, frame in bite_clip_all_frames.items():\n",
        "    if len(current_clip) < frames_per_clip:\n",
        "        current_clip.append(frame)  # Collect frames for the current clip\n",
        "    else:  # If the current clip is full, add it to the list of clips and start a new clip\n",
        "        first_index = i - (frames_per_clip * frame_step)\n",
        "        last_index = i - 1\n",
        "        is_bite = False\n",
        "        for bite_frame_index in bite_frame_indexes:\n",
        "            if first_index <= bite_frame_index <= last_index:\n",
        "                is_bite = True\n",
        "                break\n",
        "        if is_bite:\n",
        "            bites.append(tf.cast(current_clip, tf.float32))\n",
        "        else:\n",
        "            non_bites.append(tf.cast(current_clip, tf.float32))\n",
        "        current_clip = [frame]\n",
        "\n",
        "print('Bites:', len(bites))\n",
        "print('Non bites:', len(non_bites))\n",
        "\n",
        "train_bites = bites[:int(len(bites) * 0.8)]\n",
        "train_non_bites = non_bites[:int(len(non_bites) * 0.8)]\n",
        "test_bites = bites[int(len(bites) * 0.8):]\n",
        "test_non_bites = non_bites[int(len(non_bites) * 0.8):]\n",
        "\n",
        "print('Train bites:', len(train_bites))\n",
        "print('Train non bites:', len(train_non_bites))\n",
        "print('Test bites:', len(test_bites))\n",
        "print('Test non bites:', len(test_non_bites))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_bites[0].numpy().dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmpU1cG6_k0f",
        "outputId": "67e56848-ea90-43a4-81c7-c38ed9bfe484"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([50, 224, 224, 3])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# jumpingjack=load_gif(jumpingjack_path)\n",
        "# jumpingjack.shape\n",
        "bite_clip.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "# Get top_k labels and probabilities\n",
        "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Outputs the top k model labels and probabilities on the given video.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    k: the number of top predictions to select.\n",
        "    label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "  Returns:\n",
        "    a tuple of the top-k labels and probabilities.\n",
        "  \"\"\"\n",
        "  # Sort predictions to find top_k\n",
        "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
        "  # collect the labels of top_k predictions\n",
        "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
        "  # decode lablels\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "  # top_k probabilities of the predictions\n",
        "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
        "  return tuple(zip(top_labels, top_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWbdLiRsD-Yl"
      },
      "source": [
        "# Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK-wcPK0_nsF",
        "outputId": "ab33ab4b-b778-49aa-9764-0040a1e00b08"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "id = 'a0'\n",
        "mode = 'base'\n",
        "version = '3'\n",
        "hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
        "model = hub.load(hub_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a6v7sxmiB8fX",
        "outputId": "7db298bb-2b18-4c5e-d05c-0a6f065273bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "signature_wrapper(*, image)\n",
            "  Args:\n",
            "    image: float32 Tensor, shape=(None, None, None, None, 3)\n",
            "  Returns:\n",
            "    {'classifier_head': <1>}\n",
            "      <1>: float32 Tensor, shape=(None, 600)\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "sig = model.signatures['serving_default']\n",
        "print(sig.pretty_printed_signature())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wQuop-l8B_BF",
        "outputId": "d9f1a7f1-50b2-4b32-9af4-aa6557c4dc3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'classifier_head': <tf.Tensor: shape=(1, 600), dtype=float32, numpy=\n",
              " array([[-1.92758405e+00, -5.88330507e+00, -2.53679007e-01,\n",
              "         -2.52303123e-01, -5.84325075e+00,  2.47122097e+00,\n",
              "          5.24489880e+00,  5.84276104e+00,  2.61101818e+00,\n",
              "          7.57387924e+00, -1.57510221e+00, -3.12703443e+00,\n",
              "         -6.11972761e+00,  8.09002972e+00,  5.55060744e-01,\n",
              "         -4.27211332e+00, -6.54242945e+00, -3.32864523e+00,\n",
              "          1.90738440e+00,  4.52020693e+00, -1.74083507e+00,\n",
              "         -1.75236440e+00,  2.11266088e+00, -6.73124909e-01,\n",
              "          3.03988719e+00,  2.37972069e+00,  4.97288179e+00,\n",
              "          1.02900493e+00,  1.52709949e+00,  3.64214563e+00,\n",
              "          4.56970453e+00, -3.86290699e-01, -3.78517151e+00,\n",
              "         -2.77088547e+00,  1.74946249e-01, -3.30913949e+00,\n",
              "         -3.13091326e+00, -4.54574251e+00, -1.93967581e+00,\n",
              "         -8.58452320e+00, -2.43540955e+00, -3.12130404e+00,\n",
              "         -1.21681368e+00,  2.59994054e+00,  5.90099573e-01,\n",
              "         -5.32514381e+00,  1.20705509e+00, -1.58492732e+00,\n",
              "         -4.04232883e+00,  4.29580748e-01, -1.48151827e+00,\n",
              "          5.14013863e+00, -1.28086448e-01,  6.16685033e-01,\n",
              "         -1.05130982e+00, -4.33778942e-01, -8.93903971e-02,\n",
              "          2.04380083e+00,  2.56552982e+00,  4.86000717e-01,\n",
              "          5.42611790e+00, -4.08608818e+00, -5.94328356e+00,\n",
              "          1.38134658e+00,  2.27594090e+00,  2.95953989e+00,\n",
              "          1.47742724e+00,  3.07672203e-01, -9.57700610e-01,\n",
              "         -6.76300383e+00, -5.18184757e+00,  7.92998433e-01,\n",
              "         -9.61183369e-01, -3.97145808e-01, -2.64264727e+00,\n",
              "         -1.01211905e+00,  1.77323306e+00, -6.75146997e-01,\n",
              "         -6.93996859e+00,  3.41308403e+00, -4.49086094e+00,\n",
              "          4.79252195e+00,  4.24730062e+00,  4.86549234e+00,\n",
              "          3.69290638e+00,  1.54521775e+00,  1.45995021e-01,\n",
              "          7.64010191e-01, -7.61730003e+00, -3.44718575e+00,\n",
              "          3.05710578e+00,  3.25824022e+00,  3.54974461e+00,\n",
              "          9.47121334e+00, -6.56492949e+00, -5.36879921e+00,\n",
              "          5.55223179e+00, -5.01018095e+00, -9.64473009e-01,\n",
              "         -2.91998458e+00, -3.81302476e+00,  2.06363702e+00,\n",
              "         -9.83592093e-01, -2.97397017e+00, -3.37964010e+00,\n",
              "         -1.49032593e+00,  2.38101673e+00, -5.71666300e-01,\n",
              "          3.77906060e+00,  4.48844004e+00,  2.42474937e+00,\n",
              "         -5.36248386e-01,  7.50516462e+00, -5.02270937e+00,\n",
              "          4.07819510e+00,  6.36075830e+00,  3.54882383e+00,\n",
              "         -3.45504069e+00,  4.50888920e+00, -1.06228236e-02,\n",
              "         -1.24493313e+00,  4.46047974e+00,  6.39922523e+00,\n",
              "         -4.27709222e-01, -1.09135079e+00,  2.90733576e-03,\n",
              "          2.01416826e+00, -6.16131735e+00, -6.24701023e+00,\n",
              "         -1.31944609e+00,  1.85641423e-01, -5.89553785e+00,\n",
              "          1.52106142e+00,  4.42163801e+00, -1.96562660e+00,\n",
              "          2.87777567e+00, -2.69746208e+00,  4.91371250e+00,\n",
              "         -1.31905305e+00,  2.15965772e+00, -7.59226131e+00,\n",
              "          1.89163673e+00,  1.64753869e-01,  3.25328946e+00,\n",
              "          7.80417252e+00,  5.49420071e+00, -8.81601930e-01,\n",
              "         -1.70806348e-02,  1.12793952e-01, -8.36962044e-01,\n",
              "          7.60882914e-01,  8.31010103e-01, -3.16611695e+00,\n",
              "         -6.09136939e-01, -4.33514023e+00, -3.45661938e-01,\n",
              "          2.28846002e+00, -4.84256649e+00,  2.03789473e+00,\n",
              "          2.84527838e-02,  5.27869844e+00,  2.42837811e+00,\n",
              "          3.40088344e+00,  1.34689188e+00, -1.45218921e+00,\n",
              "         -1.58053589e+00,  4.99596643e+00, -4.30454063e+00,\n",
              "         -1.17572522e+00,  5.14537144e+00,  6.11910725e+00,\n",
              "          3.06529093e+00, -1.52759266e+00, -3.57462335e+00,\n",
              "         -3.07066536e+00,  1.57246900e+00,  7.50082254e-01,\n",
              "         -3.70041549e-01, -4.91127348e+00,  6.02103806e+00,\n",
              "          6.26045799e+00,  2.58078313e+00, -2.76234698e+00,\n",
              "         -4.69721413e+00,  8.18201065e-01, -3.95689511e+00,\n",
              "         -1.41582930e+00, -7.80330360e-01,  3.60834241e-01,\n",
              "          4.06067753e+00,  1.80128479e+00,  1.18441117e+00,\n",
              "         -2.75863600e+00, -5.50629997e+00, -1.22928828e-01,\n",
              "         -3.88135463e-02,  1.50255489e+00, -2.16791010e+00,\n",
              "          2.93743086e+00,  2.23821402e+00, -6.87481928e+00,\n",
              "          3.59704643e-02, -5.26522398e-01, -1.51148784e+00,\n",
              "         -3.23488855e+00, -2.54878759e+00,  7.95758867e+00,\n",
              "          4.44286299e+00,  3.56028175e+00,  3.67197943e+00,\n",
              "         -9.71232295e-01,  5.80853271e+00,  4.07019520e+00,\n",
              "         -4.06729603e+00,  6.59910250e+00,  3.61777925e+00,\n",
              "          5.01707363e+00, -2.66425920e+00, -4.13866377e+00,\n",
              "         -1.07671928e+00,  3.63316655e+00, -2.65093386e-01,\n",
              "          7.77771771e-01, -1.68835354e+00, -2.67754650e+00,\n",
              "         -4.97416592e+00,  3.05594850e+00,  8.46058559e+00,\n",
              "         -4.90944237e-01,  3.58107924e-01, -1.20673609e+00,\n",
              "         -3.55046725e+00, -2.12509394e+00, -4.29709435e+00,\n",
              "         -4.12324905e+00, -3.20965791e+00,  2.78922987e+00,\n",
              "         -2.37815046e+00,  4.61537331e-01, -8.24806881e+00,\n",
              "         -2.73100758e+00,  3.63650274e+00,  2.51922107e+00,\n",
              "         -1.29880447e+01, -3.43548965e+00, -8.12788773e+00,\n",
              "         -9.47773218e-01,  2.33958197e+00, -1.20114636e+00,\n",
              "         -3.41329670e+00, -6.67471218e+00, -1.43872678e-01,\n",
              "          9.40567970e-01, -5.69383669e+00, -4.48999166e+00,\n",
              "          9.33489799e-02, -2.39700699e+00,  2.12447262e+00,\n",
              "          1.35952199e+00,  2.04508829e+00, -3.55127454e-03,\n",
              "         -1.91156816e+00,  5.29531956e+00, -5.87449491e-01,\n",
              "         -3.00010419e+00,  2.45449829e+00, -7.63804817e+00,\n",
              "         -1.29484165e+00, -6.30950403e+00, -2.08559895e+00,\n",
              "         -1.49473870e+00, -4.25926828e+00,  1.33215785e+00,\n",
              "         -1.70207143e+00,  7.29701042e+00,  1.66115224e-01,\n",
              "          3.13205838e+00, -2.60826111e+00, -5.07018948e+00,\n",
              "          3.05238438e+00, -4.55658555e-01,  5.56719923e+00,\n",
              "         -8.93278122e-02,  2.95055509e+00,  8.71981812e+00,\n",
              "          6.61955881e+00,  6.46454668e+00,  3.53325582e+00,\n",
              "         -4.94493723e-01, -4.82727194e+00,  2.02570572e-01,\n",
              "         -8.19291174e-01,  1.39128387e-01,  8.57232285e+00,\n",
              "         -5.79134035e+00, -3.50169063e+00,  4.85265064e+00,\n",
              "          2.55861473e+00, -4.64388275e+00, -1.97635329e+00,\n",
              "         -1.79389882e+00, -6.41751528e+00, -9.11844015e-01,\n",
              "          3.77939796e+00, -4.69856358e+00, -1.48796654e+00,\n",
              "         -2.48705626e+00, -7.16145420e+00,  1.68061340e+00,\n",
              "         -1.85132205e-01, -5.84936094e+00,  1.85256839e-01,\n",
              "          2.92734241e+00,  1.09507263e+00,  2.63600349e+00,\n",
              "          1.37338352e+00, -1.05956686e+00, -1.82754815e+00,\n",
              "         -5.88286519e-01, -1.42338228e+00,  3.93724489e+00,\n",
              "          1.11594331e+00,  2.78045893e+00, -1.71855211e+00,\n",
              "          3.61331081e+00, -3.82725048e+00, -5.35503674e+00,\n",
              "         -1.04855275e+00, -2.03969669e+00,  4.93091822e-01,\n",
              "         -5.08925009e+00, -2.30405164e+00, -2.16338110e+00,\n",
              "          8.36419773e+00,  1.97886968e+00, -3.16702867e+00,\n",
              "          1.90720618e+00,  2.43569422e+00,  5.22651768e+00,\n",
              "          4.25121832e+00,  1.19419277e-01, -3.32271791e+00,\n",
              "          4.03996229e+00,  2.25659752e+00,  5.96562338e+00,\n",
              "         -2.23493576e+00,  3.63983536e+00,  5.66460013e-01,\n",
              "         -2.50791597e+00,  9.13126564e+00,  6.96209574e+00,\n",
              "         -4.83504248e+00,  3.10066724e+00, -7.54930449e+00,\n",
              "          4.37331295e+00,  1.44858170e+00,  1.33598506e+00,\n",
              "          6.51636362e-01, -3.48952436e+00, -5.22509217e-02,\n",
              "          1.12505674e+00, -6.35480738e+00, -3.65257549e+00,\n",
              "          4.22302580e+00, -4.29052019e+00,  5.98832011e-01,\n",
              "          2.92119646e+00,  3.06628764e-01, -3.61217451e+00,\n",
              "          1.05455613e+00,  5.39694130e-01, -4.13635045e-01,\n",
              "         -9.59566712e-01,  3.42568457e-01, -1.30020678e-01,\n",
              "         -1.07569790e+00,  1.90632081e+00,  1.45125175e+00,\n",
              "         -5.59198260e-02, -2.62415409e+00,  6.07549906e-01,\n",
              "         -1.21380544e+00,  3.43076181e+00, -4.37781477e+00,\n",
              "         -7.09182143e-01,  4.67992783e-01,  3.95712554e-01,\n",
              "          5.33936210e-02,  8.62329960e+00, -1.27371502e+00,\n",
              "          5.51411819e+00,  5.10442400e+00,  3.69929338e+00,\n",
              "         -2.40455031e+00, -5.51660872e+00, -3.76391411e+00,\n",
              "         -8.43613529e+00,  6.71839046e+00,  6.42244768e+00,\n",
              "          8.09115052e-01, -2.07597762e-02, -8.97139013e-01,\n",
              "         -5.30548763e+00, -5.51872587e+00,  4.70217705e+00,\n",
              "         -4.36517239e+00,  4.08145380e+00,  6.10415316e+00,\n",
              "          6.90700197e+00, -3.24616647e+00, -3.49317968e-01,\n",
              "          2.56077826e-01,  1.28717589e+00, -6.05190396e-01,\n",
              "          8.62093925e-01, -7.51000261e+00, -1.33471942e+00,\n",
              "          1.50399756e+00, -4.07073212e+00,  3.79933953e-01,\n",
              "          4.18830991e-01, -3.23186064e+00,  9.30446804e-01,\n",
              "         -3.75215793e+00,  4.15332699e+00,  4.62638283e+00,\n",
              "          6.99688792e-01,  5.09132910e+00, -7.74372816e+00,\n",
              "         -3.03848100e+00,  3.74529648e+00,  3.09337354e+00,\n",
              "          7.41653967e+00, -9.76162553e-01, -4.75042105e-01,\n",
              "          5.60292387e+00, -1.64760828e-01, -1.63186371e+00,\n",
              "          3.92734694e+00, -1.04260850e+00,  9.25094318e+00,\n",
              "          6.95346451e+00,  4.39347625e-01,  3.77097201e+00,\n",
              "          5.73691845e+00, -6.34066677e+00,  3.25904012e+00,\n",
              "         -7.86456299e+00,  4.36894083e+00,  2.13087812e-01,\n",
              "         -2.26003456e+00, -4.52609634e+00, -1.72532606e+00,\n",
              "          8.78162384e-02,  1.25960016e+00, -4.54641438e+00,\n",
              "         -4.65404081e+00, -6.69680882e+00,  1.44029462e+00,\n",
              "         -2.35916281e+00,  3.97455573e-01, -1.32017577e+00,\n",
              "          1.16710796e+01, -2.12471175e+00, -3.61800051e+00,\n",
              "         -3.52319145e+00, -5.70349026e+00, -2.88488507e+00,\n",
              "          3.27089810e+00,  1.76071739e+00, -3.74426365e+00,\n",
              "         -2.49711967e+00,  7.45970154e+00, -2.44162250e+00,\n",
              "          5.70928764e+00,  1.27026963e+00, -7.45089722e+00,\n",
              "          3.75716853e+00,  1.01463497e-01,  1.68091083e+00,\n",
              "         -2.09550500e+00,  2.44534612e-01, -2.60634375e+00,\n",
              "         -1.67857778e+00,  1.77727008e+00, -1.98076618e+00,\n",
              "          6.67516518e+00, -1.17313194e+00, -4.55649853e+00,\n",
              "         -2.86981297e+00,  3.44033122e-01, -2.82864094e+00,\n",
              "          9.39000130e-01,  1.15727079e+00,  5.90941286e+00,\n",
              "         -1.00365448e+01, -3.34174037e-02,  6.21286750e-01,\n",
              "          9.70231628e+00,  7.19056368e+00,  2.22615004e+00,\n",
              "         -5.48679888e-01,  1.61560583e+00,  1.86813080e+00,\n",
              "          3.87124515e+00,  6.33043671e+00,  7.29842329e+00,\n",
              "          4.31677818e+00, -5.46292210e+00, -7.39276409e-01,\n",
              "         -4.69173968e-01, -7.25177145e+00, -7.97792196e-01,\n",
              "         -6.64739132e-01, -1.00694628e+01,  7.53638625e-01,\n",
              "         -6.60710335e-02, -6.94024086e+00, -5.69067669e+00,\n",
              "         -1.38330805e+00,  4.04487699e-01,  1.14320517e+00,\n",
              "         -2.21107459e+00, -3.63204479e+00, -2.41867113e+00,\n",
              "         -3.66615367e+00,  3.99037886e+00,  2.05308247e+00,\n",
              "          6.98449373e+00, -5.63484621e+00, -1.77632761e+00,\n",
              "         -1.86820972e+00,  1.81614828e+00, -1.00414312e+00,\n",
              "         -2.81433868e+00, -1.83632207e+00, -4.16375816e-01,\n",
              "          2.14214325e+00, -6.53543329e+00, -1.22380328e+00,\n",
              "          2.73714495e+00, -5.65945482e+00,  2.27823687e+00,\n",
              "         -4.23957443e+00,  1.95130736e-01, -3.10766983e+00,\n",
              "          1.14692678e+01, -5.11719894e+00, -3.45756650e+00,\n",
              "         -2.37803388e+00, -4.06987762e+00, -6.57599688e+00,\n",
              "          2.24309778e+00,  5.19585562e+00,  9.46873665e+00,\n",
              "          5.96229911e-01,  3.49915552e+00,  7.83887386e+00,\n",
              "         -5.30563641e+00, -1.02582335e-01,  3.96727920e-01,\n",
              "          4.43371391e+00, -4.91847992e+00,  7.37892962e+00,\n",
              "         -3.76256537e+00,  1.24125481e+00,  6.27896070e-01,\n",
              "          3.70042276e+00,  5.71573925e+00, -1.83281124e-01,\n",
              "          1.56521749e+00,  4.45541573e+00,  2.45843220e+00,\n",
              "         -2.27349377e+00,  3.17726231e+00, -2.49043489e+00,\n",
              "          8.07582664e+00, -1.10883331e+00, -4.36605740e+00,\n",
              "          4.82959366e+00, -6.24420702e-01,  3.73881650e+00,\n",
              "         -7.32584858e+00, -4.12005329e+00, -5.17248344e+00,\n",
              "         -8.83881390e-01,  6.57263756e-01, -5.65520573e+00,\n",
              "          1.37893999e+00,  6.69408894e+00, -3.88779211e+00,\n",
              "          3.16489887e+00,  2.37699366e+00, -1.57168770e+00,\n",
              "         -1.55149126e+00, -4.05425638e-01, -2.91636920e+00,\n",
              "         -6.76618934e-01,  4.38355970e+00, -2.35319901e+00]], dtype=float32)>}"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title\n",
        "sig(image = train_bites[0][tf.newaxis, :1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW1-c04dDGTe",
        "outputId": "140dbc4f-35e4-4361-c174-8b2cb4d96df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(600,)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "logits = sig(image = train_bites[1][tf.newaxis, ...])\n",
        "logits = logits['classifier_head'][0]\n",
        "\n",
        "print(logits.shape)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R0TG_cNDuDe",
        "outputId": "50b302f1-ebf7-412a-bf70-08bbac1bb4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "blowing bubble gum  : 0.234\n",
            "beatboxing          : 0.221\n",
            "chewing gum         : 0.127\n",
            "sticking tongue out : 0.034\n",
            "scrambling eggs     : 0.011\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "probs = tf.nn.softmax(logits, axis=-1)\n",
        "for label, p in get_top_k(probs):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnWLlqdmEP4S"
      },
      "source": [
        "# Stream Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afRDBbYCENck",
        "outputId": "1d57576a-7388-46c5-f61c-22d1763d22ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 36.7 s\n",
            "Wall time: 3min 7s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "id = 'a2'\n",
        "mode = 'stream'\n",
        "version = '3'\n",
        "hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
        "model = hub.load(hub_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8CoV6-dEjHR",
        "outputId": "5d4e7b77-5bbe-4e14-a69e-344a1998bcbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['call', 'init_states']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.signatures.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOQBUt4yEldt",
        "outputId": "4152abd3-be6e-4a5a-85e2-0eb6d8b83577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "signature_wrapper(*, input_shape).\n",
            "  Args:.\n",
            "    input_shape: int32 Tensor, shape=(5,).\n",
            "  Returns:.\n",
            "    {'state/b0/l0/pool_buffer': <1>, 'state/b0/l0/pool_frame_count': <2>, 'state/b0/l1/pool_buffer': <3>, 'state/b0/l1/pool_frame_count': <4>, 'state/b0/l1/stream_buffer': <5>, 'state/b0/l2/pool_buffer': <6>, 'state/b0/l2/pool_frame_count': <7>, 'state/b0/l2/stream_buffer': <8>, 'state/b1/l0/pool_buffer': <9>, 'state/b1/l0/pool_frame_count': <10>, 'state/b1/l0/stream_buffer': <11>, 'state/b1/l1/pool_buffer': <12>, 'state/b1/l1/pool_frame_count': <13>, 'state/b1/l1/stream_buffer': <14>, 'state/b1/l2/pool_buffer': <15>, 'state/b1/l2/pool_frame_count': <16>, 'state/b1/l2/stream_buffer': <17>, 'state/b1/l3/pool_buffer': <18>, 'state/b1/l3/pool_frame_count': <19>, 'state/b1/l3/stream_buffer': <20>, 'state/b1/l4/pool_buffer': <21>, 'state/b1/l4/pool_frame_count': <22>, 'state/b1/l4/stream_buffer': <23>, 'state/b2/l0/pool_buffer': <24>, 'state/b2/l0/pool_frame_count': <25>, 'state/b2/l0/stream_buffer': <26>, 'state/b2/l1/pool_buffer': <27>, 'state/b2/l1/pool_frame_count': <28>, 'state/b2/l1/stream_buffer': <29>, 'state/b2/l2/pool_buffer': <30>, 'state/b2/l2/pool_frame_count': <31>, 'state/b2/l2/stream_buffer': <32>, 'state/b2/l3/pool_buffer': <33>, 'state/b2/l3/pool_frame_count': <34>, 'state/b2/l3/stream_buffer': <35>, 'state/b2/l4/pool_buffer': <36>, 'state/b2/l4/pool_frame_count': <37>, 'state/b2/l4/stream_buffer': <38>, 'state/b3/l0/pool_buffer': <39>, 'state/b3/l0/pool_frame_count': <40>, 'state/b3/l0/stream_buffer': <41>, 'state/b3/l1/pool_buffer': <42>, 'state/b3/l1/pool_frame_count': <43>, 'state/b3/l1/stream_buffer': <44>, 'state/b3/l2/pool_buffer': <45>, 'state/b3/l2/pool_frame_count': <46>, 'state/b3/l2/stream_buffer': <47>, 'state/b3/l3/pool_buffer': <48>, 'state/b3/l3/pool_frame_count': <49>, 'state/b3/l3/stream_buffer': <50>, 'state/b3/l4/pool_buffer': <51>, 'state/b3/l4/pool_frame_count': <52>, 'state/b3/l5/pool_buffer': <53>, 'state/b3/l5/pool_frame_count': <54>, 'state/b3/l5/stream_buffer': <55>, 'state/b4/l0/pool_buffer': <56>, 'state/b4/l0/pool_frame_count': <57>, 'state/b4/l0/stream_buffer': <58>, 'state/b4/l1/pool_buffer': <59>, 'state/b4/l1/pool_frame_count': <60>, 'state/b4/l2/pool_buffer': <61>, 'state/b4/l2/pool_frame_count': <62>, 'state/b4/l3/pool_buffer': <63>, 'state/b4/l3/pool_frame_count': <64>, 'state/b4/l4/pool_buffer': <65>, 'state/b4/l4/pool_frame_count': <66>, 'state/b4/l5/pool_buffer': <67>, 'state/b4/l5/pool_frame_count': <68>, 'state/b4/l5/stream_buffer': <69>, 'state/b4/l6/pool_buffer': <70>, 'state/b4/l6/pool_frame_count': <71>, 'state/head/pool_buffer': <72>, 'state/head/pool_frame_count': <73>}.\n",
            "      <1>: float32 Tensor, shape=(None, 1, 1, 1, 40).\n",
            "      <2>: int32 Tensor, shape=(1,).\n",
            "      <3>: float32 Tensor, shape=(None, 1, 1, 1, 40).\n",
            "      <4>: int32 Tensor, shape=(1,).\n",
            "      <5>: float32 Tensor, shape=(None, 2, None, None, 40).\n",
            "      <6>: float32 Tensor, shape=(None, 1, 1, 1, 64).\n",
            "      <7>: int32 Tensor, shape=(1,).\n",
            "      <8>: float32 Tensor, shape=(None, 2, None, None, 64).\n",
            "      <9>: float32 Tensor, shape=(None, 1, 1, 1, 96).\n",
            "      <10>: int32 Tensor, shape=(1,).\n",
            "      <11>: float32 Tensor, shape=(None, 2, None, None, 96).\n",
            "      <12>: float32 Tensor, shape=(None, 1, 1, 1, 120).\n",
            "      <13>: int32 Tensor, shape=(1,).\n",
            "      <14>: float32 Tensor, shape=(None, 2, None, None, 120).\n",
            "      <15>: float32 Tensor, shape=(None, 1, 1, 1, 96).\n",
            "      <16>: int32 Tensor, shape=(1,).\n",
            "      <17>: float32 Tensor, shape=(None, 2, None, None, 96).\n",
            "      <18>: float32 Tensor, shape=(None, 1, 1, 1, 96).\n",
            "      <19>: int32 Tensor, shape=(1,).\n",
            "      <20>: float32 Tensor, shape=(None, 2, None, None, 96).\n",
            "      <21>: float32 Tensor, shape=(None, 1, 1, 1, 120).\n",
            "      <22>: int32 Tensor, shape=(1,).\n",
            "      <23>: float32 Tensor, shape=(None, 2, None, None, 120).\n",
            "      <24>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <25>: int32 Tensor, shape=(1,).\n",
            "      <26>: float32 Tensor, shape=(None, 4, None, None, 240).\n",
            "      <27>: float32 Tensor, shape=(None, 1, 1, 1, 160).\n",
            "      <28>: int32 Tensor, shape=(1,).\n",
            "      <29>: float32 Tensor, shape=(None, 2, None, None, 160).\n",
            "      <30>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <31>: int32 Tensor, shape=(1,).\n",
            "      <32>: float32 Tensor, shape=(None, 2, None, None, 240).\n",
            "      <33>: float32 Tensor, shape=(None, 1, 1, 1, 192).\n",
            "      <34>: int32 Tensor, shape=(1,).\n",
            "      <35>: float32 Tensor, shape=(None, 2, None, None, 192).\n",
            "      <36>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <37>: int32 Tensor, shape=(1,).\n",
            "      <38>: float32 Tensor, shape=(None, 2, None, None, 240).\n",
            "      <39>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <40>: int32 Tensor, shape=(1,).\n",
            "      <41>: float32 Tensor, shape=(None, 4, None, None, 240).\n",
            "      <42>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <43>: int32 Tensor, shape=(1,).\n",
            "      <44>: float32 Tensor, shape=(None, 2, None, None, 240).\n",
            "      <45>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <46>: int32 Tensor, shape=(1,).\n",
            "      <47>: float32 Tensor, shape=(None, 2, None, None, 240).\n",
            "      <48>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <49>: int32 Tensor, shape=(1,).\n",
            "      <50>: float32 Tensor, shape=(None, 2, None, None, 240).\n",
            "      <51>: float32 Tensor, shape=(None, 1, 1, 1, 144).\n",
            "      <52>: int32 Tensor, shape=(1,).\n",
            "      <53>: float32 Tensor, shape=(None, 1, 1, 1, 240).\n",
            "      <54>: int32 Tensor, shape=(1,).\n",
            "      <55>: float32 Tensor, shape=(None, 2, None, None, 240).\n",
            "      <56>: float32 Tensor, shape=(None, 1, 1, 1, 480).\n",
            "      <57>: int32 Tensor, shape=(1,).\n",
            "      <58>: float32 Tensor, shape=(None, 4, None, None, 480).\n",
            "      <59>: float32 Tensor, shape=(None, 1, 1, 1, 384).\n",
            "      <60>: int32 Tensor, shape=(1,).\n",
            "      <61>: float32 Tensor, shape=(None, 1, 1, 1, 384).\n",
            "      <62>: int32 Tensor, shape=(1,).\n",
            "      <63>: float32 Tensor, shape=(None, 1, 1, 1, 480).\n",
            "      <64>: int32 Tensor, shape=(1,).\n",
            "      <65>: float32 Tensor, shape=(None, 1, 1, 1, 480).\n",
            "      <66>: int32 Tensor, shape=(1,).\n",
            "      <67>: float32 Tensor, shape=(None, 1, 1, 1, 480).\n",
            "      <68>: int32 Tensor, shape=(1,).\n",
            "      <69>: float32 Tensor, shape=(None, 2, None, None, 480).\n",
            "      <70>: float32 Tensor, shape=(None, 1, 1, 1, 576).\n",
            "      <71>: int32 Tensor, shape=(1,).\n",
            "      <72>: float32 Tensor, shape=(None, 1, 1, 1, 640).\n",
            "      <73>: int32 Tensor, shape=(1,)\n"
          ]
        }
      ],
      "source": [
        "lines = model.signatures['init_states'].pretty_printed_signature().splitlines()\n",
        "print('.\\n'.join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sOvkll1KGSyd"
      },
      "outputs": [],
      "source": [
        "# Get top_k labels and probabilities predicted using MoViNets streaming model\n",
        "# def get_top_k_streaming_labels(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "#   \"\"\"Returns the top-k labels over an entire video sequence.\n",
        "\n",
        "#   Args:\n",
        "#     probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "#       the probability of each class on each frame.\n",
        "#     k: the number of top predictions to select.\n",
        "#     label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "#   Returns:\n",
        "#     a tuple of the top-k probabilities, labels, and logit indices\n",
        "#   \"\"\"\n",
        "#   top_categories_last = tf.argsort(probs, -1, 'DESCENDING')[-1, :1]\n",
        "#   # Sort predictions to find top_k\n",
        "#   categories = tf.argsort(probs, -1, 'DESCENDING')[:, :k]\n",
        "#   categories = tf.reshape(categories, [-1])\n",
        "\n",
        "#   counts = sorted([\n",
        "#       (i.numpy(), tf.reduce_sum(tf.cast(categories == i, tf.int32)).numpy())\n",
        "#       for i in tf.unique(categories)[0]\n",
        "#   ], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#   top_probs_idx = tf.constant([i for i, _ in counts[:k]])\n",
        "#   top_probs_idx = tf.concat([top_categories_last, top_probs_idx], 0)\n",
        "#   # find unique indices of categories\n",
        "#   top_probs_idx = tf.unique(top_probs_idx)[0][:k+1]\n",
        "#   # top_k probabilities of the predictions\n",
        "#   top_probs = tf.gather(probs, top_probs_idx, axis=-1)\n",
        "#   top_probs = tf.transpose(top_probs, perm=(1, 0))\n",
        "#   # collect the labels of top_k predictions\n",
        "#   top_labels = tf.gather(label_map, top_probs_idx, axis=0)\n",
        "#   # decode the top_k labels\n",
        "#   top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "\n",
        "#   return top_probs, top_labels, top_probs_idx\n",
        "\n",
        "def get_top_k_labels(probs, frame_i, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Returns the top-k labels over an entire video sequence.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    k: the number of top predictions to select.\n",
        "    label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "  Returns:\n",
        "    a tuple of the top-k probabilities, labels, and logit indices\n",
        "  \"\"\"\n",
        "  top_categories_last = tf.argsort(probs, -1, 'DESCENDING')[frame_i, :1]\n",
        "  # Sort predictions to find top_k\n",
        "  categories = tf.argsort(probs, -1, 'DESCENDING')[:, :k]\n",
        "  categories = tf.reshape(categories, [-1])\n",
        "\n",
        "  counts = sorted([\n",
        "      (i.numpy(), tf.reduce_sum(tf.cast(categories == i, tf.int32)).numpy())\n",
        "      for i in tf.unique(categories)[0]\n",
        "  ], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  top_probs_idx = tf.constant([i for i, _ in counts[:k]])\n",
        "  top_probs_idx = tf.concat([top_categories_last, top_probs_idx], 0)\n",
        "  # find unique indices of categories\n",
        "  top_probs_idx = tf.unique(top_probs_idx)[0][:k+1]\n",
        "  # top_k probabilities of the predictions\n",
        "  top_probs = tf.gather(probs, top_probs_idx, axis=-1)\n",
        "  top_probs = tf.transpose(top_probs, perm=(1, 0))\n",
        "  # collect the labels of top_k predictions\n",
        "  top_labels = tf.gather(label_map, top_probs_idx, axis=0)\n",
        "  # decode the top_k labels\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "\n",
        "  return top_probs, top_labels\n",
        "\n",
        "\n",
        "# Plot top_k predictions at a given time step\n",
        "def plot_streaming_top_preds_at_step(\n",
        "    step=None,\n",
        "    image=None,\n",
        "    legend_loc='lower left',\n",
        "    duration_seconds=10,\n",
        "    figure_height=500,\n",
        "    playhead_scale=0.8,\n",
        "    grid_alpha=0.3):\n",
        "  \"\"\"Generates a plot of the top video model predictions at a given time step.\n",
        "\n",
        "  Args:\n",
        "    top_probs: a tensor of shape (k, num_frames) representing the top-k\n",
        "      probabilities over all frames.\n",
        "    top_labels: a list of length k that represents the top-k label strings.\n",
        "    step: the current time step in the range [0, num_frames].\n",
        "    image: the image frame to display at the current time step.\n",
        "    legend_loc: the placement location of the legend.\n",
        "    duration_seconds: the total duration of the video.\n",
        "    figure_height: the output figure height.\n",
        "    playhead_scale: scale value for the playhead.\n",
        "    grid_alpha: alpha value for the gridlines.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of the output numpy image, figure, and axes.\n",
        "  \"\"\"\n",
        "\n",
        "#   top_probs, top_labels = get_top_k_labels(probs, step)\n",
        "\n",
        "  # find number of top_k labels and frames in the video\n",
        "  # Visualize frames and top_k probabilities of streaming video\n",
        "  fig = plt.figure(figsize=(6.5, 7), dpi=300)\n",
        "  gs = mpl.gridspec.GridSpec(8, 1)\n",
        "  ax2 = plt.subplot(gs[:-3, :])\n",
        "  ax = plt.subplot(gs[-3:, :])\n",
        "  # display the frame\n",
        "  if image is not None:\n",
        "    ax2.imshow(image, interpolation='nearest')\n",
        "    ax2.axis('off')\n",
        "  # x-axis (frame number)\n",
        "#   preview_line_x = tf.linspace(0., duration_seconds, num_frames)\n",
        "  # y-axis (top_k probabilities)\n",
        "#   preview_line_y = top_probs\n",
        "\n",
        "#   line_x = preview_line_x[:step+1]\n",
        "#   line_y = preview_line_y[:, :step+1]\n",
        "\n",
        "#   for i in range(num_labels):\n",
        "#     ax.plot(preview_line_x, preview_line_y[i], label=None, linewidth='1.5',\n",
        "#             linestyle=':', color='gray')\n",
        "#     ax.plot(line_x, line_y[i], label=top_labels[i], linewidth='2.0')\n",
        "\n",
        "\n",
        "#   ax.grid(which='major', linestyle=':', linewidth='1.0', alpha=grid_alpha)\n",
        "#   ax.grid(which='minor', linestyle=':', linewidth='0.5', alpha=grid_alpha)\n",
        "\n",
        "#   min_height = tf.reduce_min(top_probs) * playhead_scale\n",
        "#   max_height = tf.reduce_max(top_probs)\n",
        "#   ax.vlines(preview_line_x[step], min_height, max_height, colors='red')\n",
        "#   ax.scatter(preview_line_x[step], max_height, color='red')\n",
        "\n",
        "#   ax.legend(loc=legend_loc)\n",
        "\n",
        "#   plt.xlim(0, duration_seconds)\n",
        "#   plt.ylabel('Probability')\n",
        "#   plt.xlabel('Time (s)')\n",
        "#   plt.yscale('log')\n",
        "\n",
        "  final_probs = probs[step]\n",
        "  y = 0.8\n",
        "  for label, p in get_top_k(final_probs):\n",
        "    # print(f'{label:20s}: {p:.3f}')\n",
        "    # print(f'{label:20s}', end=', ')\n",
        "    plt.text(0, y, f'{label:20s}', fontsize=35, color='red')\n",
        "    y -= 0.2\n",
        "  plt.text(0, y, f'Frame: {step}', fontsize=35, color='red')\n",
        "\n",
        "#   print()\n",
        "\n",
        "#   plt.text(0, 0.8, \"Hello, World 1!\", fontsize=35, color='red')\n",
        "#   plt.text(0, 0.6, \"Hello, World 2!\", fontsize=35, color='red')\n",
        "#   plt.text(0, 0.4, \"Hello, World 3!\", fontsize=35, color='red')\n",
        "#   plt.text(0, 0.2, \"Hello, World 4!\", fontsize=35, color='red')\n",
        "#   plt.text(0, 0, \"Hello, World!5\", fontsize=35, color='red')\n",
        "\n",
        "\n",
        "  fig.tight_layout()\n",
        "  fig.canvas.draw()\n",
        "\n",
        "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close()\n",
        "\n",
        "  figure_width = int(figure_height * data.shape[1] / data.shape[0])\n",
        "  image = PIL.Image.fromarray(data).resize([figure_width, figure_height])\n",
        "  image = np.array(image)\n",
        "\n",
        "  return image\n",
        "\n",
        "# Plotting top_k predictions from MoViNets streaming model\n",
        "def plot_streaming_top_preds(\n",
        "    probs,\n",
        "    video,\n",
        "    top_k=5,\n",
        "    video_fps=25.,\n",
        "    figure_height=500,\n",
        "    use_progbar=True):\n",
        "  \"\"\"Generates a video plot of the top video model predictions.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    video: the video to display in the plot.\n",
        "    top_k: the number of top predictions to select.\n",
        "    video_fps: the input video fps.\n",
        "    figure_fps: the output video fps.\n",
        "    figure_height: the height of the output video.\n",
        "    use_progbar: display a progress bar.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array representing the output video.\n",
        "  \"\"\"\n",
        "  # select number of frames per second\n",
        "  video_fps = 8.\n",
        "  # select height of the image\n",
        "  figure_height = 500\n",
        "  # number of time steps of the given video\n",
        "  steps = video.shape[0]\n",
        "  # estimate duration of the video (in seconds)\n",
        "  duration = steps / video_fps\n",
        "  # estimate top_k probabilities and corresponding labels\n",
        "#   top_probs, top_labels, _ = get_top_k_streaming_labels(probs, k=top_k)\n",
        "\n",
        "  images = []\n",
        "  step_generator = tqdm.trange(steps) if use_progbar else range(steps)\n",
        "  for i in step_generator:\n",
        "    image = plot_streaming_top_preds_at_step(\n",
        "        step=i,\n",
        "        image=video[i],\n",
        "        duration_seconds=duration,\n",
        "        figure_height=figure_height,\n",
        "    )\n",
        "    images.append(image)\n",
        "\n",
        "  return np.array(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "batch_size = 8\n",
        "num_frames = 6\n",
        "\n",
        "class ClipGenerator:\n",
        "    def __init__(self, training):\n",
        "        self.training = training\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.training:\n",
        "            all_clips = [(clip, 0) for clip in train_non_bites] + [(clip, 1) for clip in train_bites]\n",
        "        else:\n",
        "            all_clips = [(clip, 0) for clip in test_non_bites] + [(clip, 1) for clip in test_bites]\n",
        "        random.shuffle(all_clips)\n",
        "        for clip, label in all_clips:\n",
        "            yield clip, label\n",
        "\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(ClipGenerator(True), output_signature = output_signature)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(ClipGenerator(False), output_signature = output_signature)\n",
        "test_ds = test_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 0 0 0 1 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 0 0 0 1 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([1 0 0 0 0 1 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 1 0 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 1 0 1 0 0 0], shape=(8,), dtype=int16)\n",
            "Shape: (8, 6, 224, 224, 3)\n",
            "Label: (8,)\n"
          ]
        }
      ],
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)\n",
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skip to here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TV_ixbHLGUY1"
      },
      "outputs": [],
      "source": [
        "init_states = model.init_states(bite_clip[tf.newaxis].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv9Ai3XtGVrT",
        "outputId": "e1895ced-910f-4a2f-a773-5864a633ca9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0/75\n",
            "playing squash or racquetball, scrubbing face, applying cream, lunge, zumba, \n",
            "1/75\n",
            "answering questions, yoga, news anchoring, raising eyebrows, attending conference, \n",
            "2/75\n",
            "answering questions, chewing gum, yoga, news anchoring, burping, \n",
            "3/75\n",
            "chewing gum, answering questions, eating burger, tasting food, eating chips, \n",
            "4/75\n",
            "chewing gum, answering questions, eating burger, tasting food, eating chips, \n",
            "5/75\n",
            "chewing gum, answering questions, eating burger, eating watermelon, tasting food, \n",
            "6/75\n",
            "answering questions, chewing gum, eating burger, eating watermelon, attending conference, \n",
            "7/75\n",
            "answering questions, chewing gum, singing, attending conference, eating watermelon, \n",
            "8/75\n",
            "answering questions, eating watermelon, attending conference, chewing gum, tasting food, \n",
            "9/75\n",
            "eating watermelon, tasting food, sucking lolly, eating chips, eating carrots, \n",
            "10/75\n",
            "tasting food, sucking lolly, eating watermelon, eating carrots, eating ice cream, \n",
            "11/75\n",
            "tasting food, sucking lolly, eating ice cream, eating carrots, eating cake, \n",
            "12/75\n",
            "sucking lolly, tasting food, eating ice cream, eating carrots, eating cake, \n",
            "13/75\n",
            "sucking lolly, tasting food, eating ice cream, brushing teeth, eating cake, \n",
            "14/75\n",
            "brushing teeth, sucking lolly, eating ice cream, tasting food, eating cake, \n",
            "15/75\n",
            "brushing teeth, sucking lolly, eating ice cream, eating cake, tasting food, \n",
            "16/75\n",
            "brushing teeth, eating ice cream, sucking lolly, eating cake, tasting food, \n",
            "17/75\n",
            "brushing teeth, eating ice cream, eating cake, sucking lolly, tasting food, \n",
            "18/75\n",
            "eating ice cream, eating cake, brushing teeth, sucking lolly, tasting food, \n",
            "19/75\n",
            "eating cake, eating ice cream, brushing teeth, tasting food, sucking lolly, \n",
            "20/75\n",
            "whistling, lunge, answering questions, trimming or shaving beard, applying cream, \n",
            "21/75\n",
            "answering questions, chewing gum, whistling, eating chips, eating burger, \n",
            "22/75\n",
            "eating chips, chewing gum, eating burger, eating hotdog, blowing bubble gum, \n",
            "23/75\n",
            "eating chips, chewing gum, eating hotdog, eating burger, blowing bubble gum, \n",
            "24/75\n",
            "eating chips, chewing gum, eating hotdog, eating doughnuts, eating burger, \n",
            "25/75\n",
            "eating chips, chewing gum, eating hotdog, eating doughnuts, eating burger, \n",
            "26/75\n",
            "eating chips, chewing gum, eating hotdog, eating doughnuts, eating burger, \n",
            "27/75\n",
            "eating chips, chewing gum, eating hotdog, eating doughnuts, eating burger, \n",
            "28/75\n",
            "eating chips, chewing gum, eating hotdog, eating doughnuts, eating burger, \n",
            "29/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "30/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "31/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "32/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "33/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "34/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "35/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "36/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "37/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "38/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "39/75\n",
            "eating chips, chewing gum, eating doughnuts, eating hotdog, eating burger, \n",
            "40/75\n",
            "whistling, lunge, playing squash or racquetball, staring, waiting in line, \n",
            "41/75\n",
            "whistling, staring, answering questions, raising eyebrows, burping, \n",
            "42/75\n",
            "answering questions, whistling, staring, chewing gum, raising eyebrows, \n",
            "43/75\n",
            "answering questions, whistling, burping, staring, chewing gum, \n",
            "44/75\n",
            "answering questions, burping, whistling, chewing gum, staring, \n",
            "45/75\n",
            "answering questions, burping, whistling, eating watermelon, chewing gum, \n",
            "46/75\n",
            "answering questions, burping, eating watermelon, eating burger, eating doughnuts, \n",
            "47/75\n",
            "answering questions, burping, eating watermelon, eating burger, eating doughnuts, \n",
            "48/75\n",
            "eating burger, eating watermelon, answering questions, eating doughnuts, burping, \n",
            "49/75\n",
            "eating burger, eating watermelon, answering questions, eating doughnuts, tasting food, \n",
            "50/75\n",
            "eating burger, eating watermelon, eating doughnuts, answering questions, tasting food, \n",
            "51/75\n",
            "eating burger, eating watermelon, eating doughnuts, tasting food, eating chips, \n",
            "52/75\n",
            "eating watermelon, eating burger, eating doughnuts, tasting food, eating chips, \n",
            "53/75\n",
            "eating watermelon, eating burger, eating doughnuts, tasting food, eating chips, \n",
            "54/75\n",
            "eating watermelon, tasting food, eating burger, eating doughnuts, eating chips, \n",
            "55/75\n",
            "eating watermelon, tasting food, eating burger, eating doughnuts, eating chips, \n",
            "56/75\n",
            "eating watermelon, tasting food, eating doughnuts, eating burger, eating chips, \n",
            "57/75\n",
            "eating watermelon, tasting food, eating doughnuts, eating chips, eating burger, \n",
            "58/75\n",
            "eating watermelon, tasting food, eating doughnuts, eating chips, eating burger, \n",
            "59/75\n",
            "eating watermelon, tasting food, eating doughnuts, eating chips, eating burger, \n",
            "60/75\n",
            "lunge, playing squash or racquetball, applying cream, scrubbing face, yoga, \n",
            "61/75\n",
            "news anchoring, yoga, staring, raising eyebrows, answering questions, \n",
            "62/75\n",
            "news anchoring, answering questions, yoga, staring, raising eyebrows, \n",
            "63/75\n",
            "answering questions, news anchoring, staring, yoga, raising eyebrows, \n",
            "64/75\n",
            "answering questions, staring, news anchoring, yoga, raising eyebrows, \n",
            "65/75\n",
            "answering questions, staring, news anchoring, yoga, raising eyebrows, \n",
            "66/75\n",
            "answering questions, staring, news anchoring, raising eyebrows, attending conference, \n",
            "67/75\n",
            "answering questions, chewing gum, staring, eating chips, news anchoring, \n",
            "68/75\n",
            "chewing gum, eating chips, blowing bubble gum, tasting food, eating burger, \n",
            "69/75\n",
            "chewing gum, eating chips, blowing bubble gum, eating burger, tasting food, \n",
            "70/75\n",
            "chewing gum, eating chips, blowing bubble gum, eating burger, tasting food, \n",
            "71/75\n",
            "chewing gum, eating chips, blowing bubble gum, tasting food, eating burger, \n",
            "72/75\n",
            "chewing gum, eating chips, blowing bubble gum, tasting food, eating burger, \n",
            "73/75\n",
            "chewing gum, eating chips, blowing bubble gum, tasting food, sticking tongue out, \n",
            "74/75\n",
            "chewing gum, eating chips, blowing bubble gum, tasting food, answering questions, \n"
          ]
        }
      ],
      "source": [
        "# Insert your video clip here\n",
        "video = 1 - bite_clip\n",
        "# video = tf.gather(video, indices=[2, 1, 0], axis=-1)\n",
        "images = tf.split(video[tf.newaxis], video.shape[0], axis=1)\n",
        "\n",
        "all_logits = []\n",
        "\n",
        "# To run on a video, pass in one frame at a time\n",
        "states = init_states\n",
        "for i, image in enumerate(images):\n",
        "  print(f'{i}/{len(images)}')\n",
        "  # predictions for each frame\n",
        "  if i > 0 and i % 20 == 0:\n",
        "    states = init_states\n",
        "  logits, states = model({**states, 'image': image})\n",
        "\n",
        "  for label, p in get_top_k(logits[0]):\n",
        "    print(label, end=', ')\n",
        "  print()\n",
        "\n",
        "  all_logits.append(logits)\n",
        "\n",
        "# concatenating all the logits\n",
        "logits = tf.concat(all_logits, 0)\n",
        "# estimating probabilities\n",
        "probs = tf.nn.softmax(logits, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0/109\n",
            "staring 5.196523, raising eyebrows 4.741813, lunge 4.3045793, spray painting 4.0851145, playing squash or racquetball 3.7177255, \n",
            "1/109\n",
            "chewing gum 9.992247, eating chips 9.763977, eating hotdog 9.054871, blowing bubble gum 8.813029, staring 8.161706, \n",
            "2/109\n",
            "chewing gum 21.292713, eating chips 20.507023, eating hotdog 18.845299, blowing bubble gum 16.13454, eating burger 15.714641, \n",
            "3/109\n",
            "chewing gum 22.756895, eating chips 22.327326, eating hotdog 20.309887, eating burger 16.99799, blowing bubble gum 16.773079, \n",
            "4/109\n",
            "chewing gum 22.73225, eating chips 22.248854, eating hotdog 20.13458, beatboxing 17.16296, eating burger 16.556318, \n",
            "5/109\n",
            "staring 5.590745, whistling 5.4148397, lunge 4.755303, raising eyebrows 4.2304707, crossing eyes 3.829576, \n",
            "6/109\n",
            "raising eyebrows 7.7490196, staring 7.7115245, chewing gum 7.644267, whistling 6.5392447, eating hotdog 6.45524, \n",
            "7/109\n",
            "chewing gum 10.532325, eating hotdog 9.142977, eating chips 9.1263685, blowing bubble gum 8.567889, eating burger 8.215074, \n",
            "8/109\n",
            "chewing gum 9.353918, eating chips 8.219544, eating hotdog 8.110937, blowing bubble gum 7.626853, eating burger 7.350472, \n",
            "9/109\n",
            "chewing gum 8.617248, eating chips 7.7033844, eating hotdog 7.433236, eating burger 6.9532905, blowing bubble gum 6.919385, \n",
            "10/109\n",
            "lunge 4.942141, news anchoring 4.346394, sweeping floor 3.7038424, mopping floor 3.6649354, waxing eyebrows 3.1637092, \n",
            "11/109\n",
            "chewing gum 6.0737095, eating chips 5.6528907, tasting food 4.9606586, eating burger 4.6714425, beatboxing 4.5066705, \n",
            "12/109\n",
            "chewing gum 5.735533, eating chips 4.8418684, beatboxing 4.453743, blowing bubble gum 4.337411, raising eyebrows 4.3306093, \n",
            "13/109\n",
            "chewing gum 7.6766562, eating chips 6.738044, blowing bubble gum 6.052132, beatboxing 5.3443236, tasting food 5.28869, \n",
            "14/109\n",
            "chewing gum 9.024986, eating chips 7.7921443, blowing bubble gum 7.3540406, tasting food 6.131459, eating burger 6.1079154, \n",
            "15/109\n",
            "lunge 4.378252, mopping floor 4.3055053, sweeping floor 4.0523086, tying shoe laces 3.973299, stretching leg 3.313466, \n",
            "16/109\n",
            "chewing gum 3.9880128, lunge 3.7539172, drooling 3.5609353, singing 3.4663343, eating chips 3.381743, \n",
            "17/109\n",
            "chewing gum 8.703548, eating chips 7.447153, blowing bubble gum 6.42501, beatboxing 5.6944532, eating burger 5.605392, \n",
            "18/109\n",
            "chewing gum 9.180448, eating chips 7.929373, blowing bubble gum 6.5581055, beatboxing 6.4839053, eating burger 6.0800705, \n",
            "19/109\n",
            "chewing gum 9.246175, eating chips 8.168858, beatboxing 7.1237774, blowing bubble gum 6.51161, eating burger 6.278486, \n",
            "20/109\n",
            "lunge 4.577892, stretching leg 3.8452969, mopping floor 3.5558426, applying cream 3.2459645, front raises 3.2018232, \n",
            "21/109\n",
            "staring 5.6564403, raising eyebrows 4.819654, sleeping 4.762919, drooling 4.7024918, dyeing eyebrows 4.239352, \n",
            "22/109\n",
            "staring 5.997336, sleeping 5.095574, drooling 4.9035254, raising eyebrows 4.647405, crying 3.9470215, \n",
            "23/109\n",
            "staring 5.34467, drooling 5.147146, whistling 4.972251, crying 4.42953, sleeping 4.1545954, \n",
            "24/109\n",
            "yawning 7.2212663, chewing gum 6.494581, drooling 6.0808687, eating chips 6.0401564, sticking tongue out 5.6383204, \n",
            "25/109\n",
            "tightrope walking 5.8986454, playing squash or racquetball 5.2556524, yarn spinning 4.3589277, lunge 4.31013, brushing teeth 4.255541, \n",
            "26/109\n",
            "brushing teeth 17.982265, sucking lolly 14.567636, putting on lipstick 12.825863, eating ice cream 10.186897, eating cake 9.182821, \n",
            "27/109\n",
            "brushing teeth 24.596754, sucking lolly 18.65209, putting on lipstick 15.940206, eating ice cream 14.401864, eating cake 12.460887, \n",
            "28/109\n",
            "brushing teeth 25.989717, sucking lolly 19.751463, eating ice cream 16.995274, putting on lipstick 16.565298, eating cake 14.672933, \n",
            "29/109\n",
            "brushing teeth 23.11931, sucking lolly 18.52667, eating ice cream 16.558937, putting on lipstick 14.8315325, eating cake 14.57754, \n",
            "30/109\n",
            "staring 6.709618, crossing eyes 5.049711, whistling 4.975616, raising eyebrows 4.9523125, lunge 4.38842, \n",
            "31/109\n",
            "chewing gum 11.447133, blowing bubble gum 10.862948, eating chips 10.211502, eating hotdog 9.953944, staring 9.772864, \n",
            "32/109\n",
            "chewing gum 22.008944, eating chips 20.979395, eating hotdog 19.737799, blowing bubble gum 18.26764, eating doughnuts 16.681065, \n",
            "33/109\n",
            "chewing gum 24.065256, eating chips 23.678305, eating hotdog 21.92006, blowing bubble gum 18.995523, eating doughnuts 18.14987, \n",
            "34/109\n",
            "chewing gum 24.261435, eating chips 24.01841, eating hotdog 22.1743, blowing bubble gum 18.480219, eating doughnuts 18.22376, \n",
            "35/109\n",
            "staring 5.752423, raising eyebrows 5.1284337, crossing eyes 4.101352, spray painting 4.059105, whistling 3.6767464, \n",
            "36/109\n",
            "staring 8.690393, chewing gum 6.8770847, raising eyebrows 6.820961, eating chips 6.7456017, eating hotdog 6.520412, \n",
            "37/109\n",
            "chewing gum 16.06594, eating chips 15.898452, eating hotdog 14.070847, eating burger 12.267597, blowing bubble gum 11.98191, \n",
            "38/109\n",
            "eating chips 18.414658, chewing gum 18.255394, eating hotdog 16.226303, eating burger 14.080345, eating doughnuts 13.663528, \n",
            "39/109\n",
            "eating chips 20.28921, chewing gum 19.909681, eating hotdog 17.993172, eating burger 15.394405, eating doughnuts 15.152672, \n",
            "40/109\n",
            "raising eyebrows 4.7802935, staring 4.773502, whistling 4.345283, lunge 4.053784, spray painting 3.931965, \n",
            "41/109\n",
            "staring 8.021588, whistling 7.8897696, raising eyebrows 6.4351015, crossing eyes 5.6746764, fixing hair 4.8097267, \n",
            "42/109\n",
            "whistling 8.355687, staring 8.118832, raising eyebrows 5.9957595, crossing eyes 5.421511, beatboxing 5.134634, \n",
            "43/109\n",
            "staring 7.804417, whistling 7.3320484, beatboxing 6.374602, raising eyebrows 5.782574, crossing eyes 5.31897, \n",
            "44/109\n",
            "beatboxing 8.441326, chewing gum 8.0282545, eating chips 7.4651084, staring 6.917331, eating hotdog 6.8084836, \n",
            "45/109\n",
            "lunge 3.9460251, scrubbing face 3.1405466, using a sledge hammer 3.1153738, whistling 2.9333997, sweeping floor 2.9327197, \n",
            "46/109\n",
            "whistling 5.3605375, beatboxing 3.6423545, staring 3.5657566, cracking neck 3.306283, using a sledge hammer 3.2747881, \n",
            "47/109\n",
            "beatboxing 5.1197414, chewing gum 4.6743917, whistling 4.4523916, eating chips 4.251879, eating burger 3.8476717, \n",
            "48/109\n",
            "beatboxing 4.8419986, chewing gum 4.532713, eating chips 4.2664165, eating burger 4.0081286, whistling 3.9900308, \n",
            "49/109\n",
            "chewing gum 5.878072, eating chips 5.632892, beatboxing 5.5372725, eating burger 5.1532984, eating hotdog 4.5688567, \n",
            "50/109\n",
            "mopping floor 5.270444, sweeping floor 4.1039467, sanding floor 3.7692952, washing hands 3.585525, making bubbles 3.469087, \n",
            "51/109\n",
            "chewing gum 6.4297104, eating chips 6.0406137, tasting food 5.0434012, eating burger 4.494221, drooling 4.478172, \n",
            "52/109\n",
            "chewing gum 10.285291, eating chips 8.900852, blowing bubble gum 7.2129498, tasting food 7.069109, eating burger 6.6103745, \n",
            "53/109\n",
            "chewing gum 10.121619, eating chips 8.501046, blowing bubble gum 7.3667355, tasting food 6.753929, eating burger 5.983404, \n",
            "54/109\n",
            "chewing gum 11.159476, eating chips 9.431446, blowing bubble gum 8.139209, tasting food 7.332797, eating burger 7.088847, \n",
            "55/109\n",
            "tying shoe laces 5.6596637, mopping floor 4.932053, sweeping floor 4.5799484, stretching leg 4.4928865, sanding floor 4.487275, \n",
            "56/109\n",
            "drooling 4.499681, braiding hair 4.1172256, sleeping 3.545139, staring 3.4314911, tying shoe laces 3.4131672, \n",
            "57/109\n",
            "chewing gum 7.247617, eating chips 5.750606, drooling 5.009248, blowing bubble gum 4.6367764, tasting food 4.5328364, \n",
            "58/109\n",
            "chewing gum 7.5462227, eating chips 6.470358, blowing bubble gum 5.097808, tasting food 5.019479, drooling 4.9952493, \n",
            "59/109\n",
            "chewing gum 7.5200324, eating chips 6.636115, blowing bubble gum 5.1905255, tasting food 5.1156445, drooling 4.911896, \n",
            "60/109\n",
            "lunge 4.1947765, scrubbing face 3.831726, tying shoe laces 3.8231454, washing hands 3.8027575, mopping floor 3.6821861, \n",
            "61/109\n",
            "singing 5.0759254, chewing gum 4.355232, tasting food 3.7565804, reading book 3.7555892, eating chips 3.557227, \n",
            "62/109\n",
            "chewing gum 6.6058025, singing 5.41333, beatboxing 5.384656, eating chips 5.344723, tasting food 4.5742702, \n",
            "63/109\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     12\u001b[0m     states \u001b[38;5;241m=\u001b[39m init_states\n\u001b[1;32m---> 13\u001b[0m logits, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, p \u001b[38;5;129;01min\u001b[39;00m get_top_k(logits[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(label, p, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:704\u001b[0m, in \u001b[0;36m_call_attribute\u001b[1;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_attribute\u001b[39m(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 704\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "video = 1 - bite_clip\n",
        "images = tf.split(video[tf.newaxis], video.shape[0], axis=1)\n",
        "\n",
        "all_logits = []\n",
        "\n",
        "# To run on a video, pass in one frame at a time\n",
        "states = init_states\n",
        "for i, image in enumerate(images):\n",
        "    print(f'{i}/{len(images)}')\n",
        "    # predictions for each frame\n",
        "    if i > 0 and i % 5 == 0:\n",
        "        states = init_states\n",
        "    logits, states = model({**states, 'image': image})\n",
        "\n",
        "    for label, p in get_top_k(logits[0]):\n",
        "        print(label, p, end=', ')\n",
        "    print()\n",
        "\n",
        "    all_logits.append(logits)\n",
        "\n",
        "# concatenating all the logits\n",
        "logits = tf.concat(all_logits, 0)\n",
        "# estimating probabilities\n",
        "probs = tf.nn.softmax(logits, axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3K5fxbJGY9u",
        "outputId": "07c43b90-1068-469b-b1b4-eac50bea5783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top_k predictions and their probablities\n",
            "\n",
            "chewing gum         : 0.118\n",
            "eating chips        : 0.058\n",
            "blowing bubble gum  : 0.052\n",
            "answering questions : 0.039\n",
            "staring             : 0.035\n"
          ]
        }
      ],
      "source": [
        "final_probs = probs[-1]\n",
        "print('Top_k predictions and their probablities\\n')\n",
        "for label, p in get_top_k(final_probs):\n",
        "  print(f'{label:20s}: {p:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG5aS_2gGcxv",
        "outputId": "8c3223f9-d00c-4dce-8b5d-43243524d3c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/75 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Evan Mazor\\AppData\\Local\\Temp\\ipykernel_17128\\89590184.py:164: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
            "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
            "100%|██████████| 75/75 [00:37<00:00,  1.97it/s]\n"
          ]
        }
      ],
      "source": [
        "# Generate a plot and output to a video tensor\n",
        "plot_video = plot_streaming_top_preds(probs, video, video_fps=8., top_k=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def save_images(images, filename, fps=20.0):\n",
        "    # Define the codec and create VideoWriter object\n",
        "    # 'XVID' is a commonly used codec. You can replace it with 'MJPG', 'X264', etc.\n",
        "    # Use 'mp4v' codec for MP4 files\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    output_file = f'{filename}.mp4'  # Output video file name\n",
        "    height, width, channels = images[0].shape\n",
        "\n",
        "    # Initialize the video writer\n",
        "    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
        "\n",
        "    # Loop through each image and write it to the video\n",
        "    for image in images:\n",
        "        video_writer.write(image)\n",
        "\n",
        "    # Release the video writer object\n",
        "    video_writer.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 224, 224, 3)"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bite_clip = np.fromfile('bite_clip.raw', dtype=np.float32) * 255\n",
        "bite_clip = bite_clip.astype(np.uint8).reshape((8, 224, 224, 3))\n",
        "bite_clip.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title\n",
        "# Get top_k labels and probabilities\n",
        "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Outputs the top k model labels and probabilities on the given video.\n",
        "\n",
        "  Args:\n",
        "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
        "      the probability of each class on each frame.\n",
        "    k: the number of top predictions to select.\n",
        "    label_map: a list of labels to map logit indices to label strings.\n",
        "\n",
        "  Returns:\n",
        "    a tuple of the top-k labels and probabilities.\n",
        "  \"\"\"\n",
        "  # Sort predictions to find top_k\n",
        "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
        "  # collect the labels of top_k predictions\n",
        "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
        "  # decode lablels\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "  # top_k probabilities of the predictions\n",
        "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
        "  return tuple(zip(top_labels, top_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "uDUjLGZrGgWB",
        "outputId": "f46e915b-53f8-4d55-ec49-87b78f822d0d"
      },
      "outputs": [],
      "source": [
        "# For gif format, set codec='gif'\n",
        "# save_images(plot_video, 'biteclip_probabilities_v4', fps=8)\n",
        "save_images(1 - (train_bites[3].numpy() * 255).astype(np.uint8), 'bite_clip', fps=5)\n",
        "# save_images((jumpingjack.numpy() * 255).astype(np.uint8), 'jjack', fps=3)\n",
        "# media.show_video(plot_video, fps=3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
