{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x249877b56f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model\n",
    "\n",
    "def create_model():\n",
    "    model_id = 'a0'\n",
    "    resolution = 224\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    backbone = movinet.Movinet(model_id=model_id)\n",
    "    backbone.trainable = False\n",
    "\n",
    "    # Set num_classes=600 to load the pre-trained weights from the original model\n",
    "    model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=2)\n",
    "    model.build([1, 8, 224, 224, 3])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.load_weights('movinet_fine_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 27s 27s/step\n",
      "[[-1.1264563  1.172403 ]]\n"
     ]
    }
   ],
   "source": [
    "bite_clip = np.fromfile('bite_clip.raw', dtype=np.float32).reshape(1, 8, 224, 224, 3)\n",
    "print(bite_clip.shape)\n",
    "print(model.predict(bite_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Resize the frame to the input size expected by the MoViNet model\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Adjust size if your model uses different dimensions\n",
    "    # Normalize the frame (0-255 to 0-1)\n",
    "    frame_normalized = frame_resized / 255.0\n",
    "    return frame_normalized\n",
    "\n",
    "def get_clips_from_stream(cap, num_frames=8, stride=8):\n",
    "    frames = []\n",
    "    clip_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        clip_count += 1\n",
    "        if clip_count % stride == 0:\n",
    "            processed_frame = preprocess_frame(frame)\n",
    "            frames.append(processed_frame)\n",
    "\n",
    "        # Show the real-time camera feed\n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "        # Once we've gathered enough frames for one clip\n",
    "        if len(frames) == num_frames:\n",
    "            yield np.array(frames)\n",
    "            frames = []  # Reset for the next clip\n",
    "\n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 20s 20s/step\n",
      "[ 1.1196219 -1.0239307]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "[ 0.40332037 -0.3020206 ]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "[ 0.1806604  -0.08135715]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 11s 11s/step\n",
      "[ 0.5883975  -0.49146715]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 12s 12s/step\n",
      "[-2.2525742  2.3821332]\n",
      "Action detected!\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 11s 11s/step\n",
      "[ 0.73208445 -0.61878085]\n",
      "No action detected.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize video capture (0 for the first camera, or provide a filename for video file)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print('starting')\n",
    "\n",
    "for clip in get_clips_from_stream(cap):\n",
    "    # Add batch dimension and predict\n",
    "    clip_input = np.expand_dims(clip, axis=0)  # Shape should be (1, 8, 224, 224, 3)\n",
    "    print(clip_input.shape)\n",
    "\n",
    "    # Predict with the model\n",
    "    prediction = model.predict(clip_input)\n",
    "\n",
    "    # Analyze the prediction (for binary classification, for instance)\n",
    "    print(prediction[0])\n",
    "    if prediction[0][0] < prediction[0][1]:  # Adjust threshold if necessary\n",
    "        print(\"Action detected!\")\n",
    "    else:\n",
    "        print(\"No action detected.\")\n",
    "\n",
    "    # You can add visualization or further processing here\n",
    "\n",
    "# Release the capture when done\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
