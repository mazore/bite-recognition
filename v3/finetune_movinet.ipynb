{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model\n",
    "\n",
    "frame_step = 8\n",
    "frames_per_clip = 8\n",
    "output_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_frames():\n",
    "    video_capture = cv2.VideoCapture('../raw_session.mp4')\n",
    "    frames = {}\n",
    "    success, frame = video_capture.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        if count % frame_step == 0:  # Extract frame every 'frame_rate' frames\n",
    "            # frame = frame[0:720, 280:1000]\n",
    "            # frame = cv2.resize(frame, (224, 224))\n",
    "            frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "            frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "            frames[count] = frame\n",
    "            if count % 800 == 0:\n",
    "                print(count)\n",
    "            # cv2.imshow('frame', frame)\n",
    "            # if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            #     break\n",
    "        success, frame = video_capture.read()\n",
    "        count += 1\n",
    "    video_capture.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "800\n",
      "1600\n",
      "2400\n",
      "3200\n",
      "4000\n",
      "4800\n",
      "5600\n",
      "6400\n",
      "7200\n"
     ]
    }
   ],
   "source": [
    "all_frames = extract_all_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[508, 947, 1201, 1578, 1959, 2492, 2881, 3136, 3377, 3727, 5149, 6310, 6848]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('bite_frame_indexes.json') as f:\n",
    "    bite_frame_indexes = json.load(f)\n",
    "bite_frame_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bites: 13\n",
      "Non bites: 110\n",
      "Train bites: 10\n",
      "Train non bites: 88\n",
      "Test bites: 3\n",
      "Test non bites: 22\n"
     ]
    }
   ],
   "source": [
    "bites = []\n",
    "non_bites = []\n",
    "current_clip = []\n",
    "for i, frame in all_frames.items():\n",
    "    if len(current_clip) < frames_per_clip:\n",
    "        current_clip.append(frame)  # Collect frames for the current clip\n",
    "    else:  # If the current clip is full, add it to the list of clips and start a new clip\n",
    "        first_index = i - (frames_per_clip * frame_step)\n",
    "        last_index = i - 1\n",
    "        is_bite = False\n",
    "        for bite_frame_index in bite_frame_indexes:\n",
    "            if first_index <= bite_frame_index <= last_index:\n",
    "                is_bite = True\n",
    "                break\n",
    "        if is_bite:\n",
    "            bites.append(current_clip)\n",
    "        else:\n",
    "            non_bites.append(current_clip)\n",
    "        current_clip = [frame]\n",
    "\n",
    "print('Bites:', len(bites))\n",
    "print('Non bites:', len(non_bites))\n",
    "\n",
    "train_bites = bites[:int(len(bites) * 0.8)]\n",
    "train_non_bites = non_bites[:int(len(non_bites) * 0.8)]\n",
    "test_bites = bites[int(len(bites) * 0.8):]\n",
    "test_non_bites = non_bites[int(len(non_bites) * 0.8):]\n",
    "\n",
    "print('Train bites:', len(train_bites))\n",
    "print('Train non bites:', len(train_non_bites))\n",
    "print('Test bites:', len(test_bites))\n",
    "print('Test non bites:', len(test_non_bites))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(images, output_path, fps=30):\n",
    "    # Get the height, width, and number of channels of the images\n",
    "    height, width, layers = images[0].shape\n",
    "\n",
    "    # Define the codec and create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID', 'MJPG', etc.\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        out.write(image)  # Write each image to the video\n",
    "\n",
    "    out.release()  # Release the VideoWriter object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8) for _ in range(60)]  # 60 random images\n",
    "images = non_bites[1]\n",
    "images = np.array(images)[..., [2, 1, 0]]\n",
    "save_video(images, 'output_video.mp4', fps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_frames = 8\n",
    "\n",
    "class ClipGenerator:\n",
    "    def __init__(self, training):\n",
    "        self.training = training\n",
    "\n",
    "    def __call__(self):\n",
    "        if self.training:\n",
    "            all_clips = [(clip, 0) for clip in train_non_bites] + [(clip, 1) for clip in train_bites]\n",
    "        else:\n",
    "            all_clips = [(clip, 0) for clip in test_non_bites] + [(clip, 1) for clip in test_bites]\n",
    "        random.shuffle(all_clips)\n",
    "        for clip, label in all_clips:\n",
    "            yield clip, label\n",
    "\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(ClipGenerator(True), output_signature = output_signature)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(ClipGenerator(False), output_signature = output_signature)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 0 1 1 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 0 0 0 1 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 0 0 0 1 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 1 0 0 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 1 1 0 0 0 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([0 0 0 0 0 0 1 0], shape=(8,), dtype=int16)\n",
      "Shape: (8, 8, 224, 224, 3)\n",
      "Label: (8,)\n"
     ]
    }
   ],
   "source": [
    "for frames, labels in train_ds.take(10):\n",
    "  print(labels)\n",
    "print(f\"Shape: {frames.shape}\")\n",
    "print(f\"Label: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x190d8f14610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz\n",
    "# !tar -xvf movinet_a0_base.tar.gz\n",
    "\n",
    "model_id = 'a0'\n",
    "resolution = 224\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "backbone = movinet.Movinet(model_id=model_id)\n",
    "backbone.trainable = False\n",
    "\n",
    "# Set num_classes=600 to load the pre-trained weights from the original model\n",
    "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
    "model.build([None, None, None, None, 3])\n",
    "\n",
    "checkpoint_dir = f'movinet_{model_id}_base'\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "status = checkpoint.restore(checkpoint_path)\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
    "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "  model = movinet_model.MovinetClassifier(\n",
    "      backbone=backbone,\n",
    "      num_classes=num_classes)\n",
    "  model.build([batch_size, frames_per_clip, resolution, resolution, 3])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classifier(batch_size, frames_per_clip, resolution, backbone, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0, 1: 8.461538461538462}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "class_weights = {0: 1.0, 1: len(non_bites) / len(bites)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "13/13 [==============================] - 610s 47s/step - loss: 0.8684 - accuracy: 0.7551 - val_loss: 0.2640 - val_accuracy: 0.8800\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 558s 44s/step - loss: 0.5625 - accuracy: 0.9082 - val_loss: 0.2534 - val_accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(train_ds,\n",
    "                    validation_data=test_ds,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_freq=1,\n",
    "                    class_weight=class_weights,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('movinet_fine_tuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Resize the frame to the input size expected by the MoViNet model\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Adjust size if your model uses different dimensions\n",
    "    # Normalize the frame (0-255 to 0-1)\n",
    "    frame_normalized = frame_resized / 255.0\n",
    "    return frame_normalized\n",
    "\n",
    "def get_clips_from_stream(cap, num_frames=8, stride=8):\n",
    "    frames = []\n",
    "    clip_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        clip_count += 1\n",
    "        if clip_count % stride == 0:\n",
    "            processed_frame = preprocess_frame(frame)\n",
    "            frames.append(processed_frame)\n",
    "\n",
    "        # Show the real-time camera feed\n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "        # Once we've gathered enough frames for one clip\n",
    "        if len(frames) == num_frames:\n",
    "            yield np.array(frames)\n",
    "            frames = []  # Reset for the next clip\n",
    "\n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "[ 0.6878443 -0.6533364]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "[-0.15973273  0.27262238]\n",
      "Action detected!\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "[-2.335931   2.4199948]\n",
      "Action detected!\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "[ 0.8604256  -0.76669306]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "[-1.252853   1.4119458]\n",
      "Action detected!\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "[ 0.9320724  -0.83366436]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "[ 0.5351406 -0.4519203]\n",
      "No action detected.\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "[-0.82155776  0.9903483 ]\n",
      "Action detected!\n",
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "[-0.26073048  0.3499933 ]\n",
      "Action detected!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize video capture (0 for the first camera, or provide a filename for video file)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print('starting')\n",
    "\n",
    "for clip in get_clips_from_stream(cap):\n",
    "    # Add batch dimension and predict\n",
    "    clip_input = np.expand_dims(clip, axis=0)  # Shape should be (1, 8, 224, 224, 3)\n",
    "    print(clip_input.shape)\n",
    "\n",
    "    # Predict with the model\n",
    "    prediction = model.predict(clip_input)\n",
    "\n",
    "    # Analyze the prediction (for binary classification, for instance)\n",
    "    print(prediction[0])\n",
    "    if prediction[0][0] < prediction[0][1]:  # Adjust threshold if necessary\n",
    "        print(\"Action detected!\")\n",
    "    else:\n",
    "        print(\"No action detected.\")\n",
    "\n",
    "    # You can add visualization or further processing here\n",
    "\n",
    "# Release the capture when done\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    video_capture.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D62FE680>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D62FE680>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D63B08B0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D63B08B0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190C15B1F30>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190C15B1F30>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D64F3FA0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D64F3FA0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D6505DB0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D6505DB0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190C15C0A00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190C15C0A00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D62E6050>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D62E6050>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D66BE2C0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D66BE2C0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D6737E50>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D6737E50>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D67D2500>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D67D2500>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D687CC70>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D687CC70>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D68F7820>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D68F7820>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D69BD480>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D69BD480>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D6A68940>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D6A68940>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D8F59F00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x00000190D8F59F00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000190C16180D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000190C16180D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.TemporalSoftmaxPool object at 0x00000190ED1E39D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.TemporalSoftmaxPool object at 0x00000190ED1E39D0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as init_states, head_layer_call_fn, head_layer_call_and_return_conditional_losses, classifier_layer_call_fn, classifier_layer_call_and_return_conditional_losses while saving (showing 5 of 691). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\EVANMA~1\\AppData\\Local\\Temp\\tmptoidoco5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\EVANMA~1\\AppData\\Local\\Temp\\tmptoidoco5\\assets\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: AvgPool3D\nDetails:\n\ttf.AvgPool3D(tensor<?x?x?x?x32xf32>) -> (tensor<?x?x?x?x32xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\ttf.AvgPool3D(tensor<?x?x?x?x56xf32>) -> (tensor<?x?x?x?x56xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\ttf.AvgPool3D(tensor<?x?x?x?x8xf32>) -> (tensor<?x?x?x?x8xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n\u001b[0;32m     10\u001b[0m converter\u001b[38;5;241m.\u001b[39mexperimental_new_converter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Save the TFLite model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:930\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_and_export_metrics(convert_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:908\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m    907\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m--> 908\u001b[0m result \u001b[38;5;241m=\u001b[39m convert_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    909\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1339\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1339\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1322\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1318\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1319\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_keras_to_saved_model(temp_dir))\n\u001b[0;32m   1320\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1322\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m   shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1132\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[1;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[0;32m   1127\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing new converter: If you encounter a problem \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1128\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease file a bug. You can opt-out \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1129\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby setting experimental_new_converter=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[1;32m-> 1132\u001b[0m result \u001b[38;5;241m=\u001b[39m _convert_graphdef(\n\u001b[0;32m   1133\u001b[0m     input_data\u001b[38;5;241m=\u001b[39mgraph_def,\n\u001b[0;32m   1134\u001b[0m     input_tensors\u001b[38;5;241m=\u001b[39minput_tensors,\n\u001b[0;32m   1135\u001b[0m     output_tensors\u001b[38;5;241m=\u001b[39moutput_tensors,\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs)\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_tflite_model(\n\u001b[0;32m   1139\u001b[0m     result, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quant_mode, quant_io\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:212\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     report_error_message(\u001b[38;5;28mstr\u001b[39m(converter_error))\n\u001b[1;32m--> 212\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m converter_error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Re-throws the exception.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:794\u001b[0m, in \u001b[0;36mconvert_graphdef\u001b[1;34m(input_data, input_tensors, output_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    791\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    792\u001b[0m     model_flags\u001b[38;5;241m.\u001b[39moutput_arrays\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mget_tensor_name(output_tensor))\n\u001b[1;32m--> 794\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_flags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversion_flags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug_info_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_mlir_converter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_mlir_converter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:311\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_data \u001b[38;5;129;01min\u001b[39;00m _metrics_wrapper\u001b[38;5;241m.\u001b[39mretrieve_collected_errors():\n\u001b[0;32m    310\u001b[0m       converter_error\u001b[38;5;241m.\u001b[39mappend_error(error_data)\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converter_error\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _run_deprecated_conversion_binary(model_flags_str,\n\u001b[0;32m    314\u001b[0m                                          conversion_flags_str, input_data_str,\n\u001b[0;32m    315\u001b[0m                                          debug_info_str)\n",
      "\u001b[1;31mConverterError\u001b[0m: c:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: error: 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097:0: note: Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: AvgPool3D\nDetails:\n\ttf.AvgPool3D(tensor<?x?x?x?x32xf32>) -> (tensor<?x?x?x?x32xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\ttf.AvgPool3D(tensor<?x?x?x?x56xf32>) -> (tensor<?x?x?x?x56xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\ttf.AvgPool3D(tensor<?x?x?x?x8xf32>) -> (tensor<?x?x?x?x8xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\n"
     ]
    }
   ],
   "source": [
    "# Suppose the input shape is (batch_size, num_frames, height, width, channels)\n",
    "# For your case with 8 frames of size 224x224 and 3 channels (RGB):\n",
    "input_shape = (1, 8, 224, 224, 3)  # 1 for batch_size, can be any batch size\n",
    "\n",
    "# Build the model with a dummy input to ensure all layers are initialized\n",
    "model.build(input_shape)\n",
    "\n",
    "# Now convert the model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.experimental_new_converter = True\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 224, 224, 3)\n",
      "1/1 [==============================] - 22s 22s/step\n",
      "[[-1.1264563  1.172403 ]]\n"
     ]
    }
   ],
   "source": [
    "# print(np.array(bites[0]).shape)\n",
    "\n",
    "bite_clip = np.array(bites[0]).astype(np.float32)\n",
    "bite_clip = np.expand_dims(bite_clip, axis=0)\n",
    "bite_clip.tofile('bite_clip.raw')\n",
    "print(bite_clip.shape)\n",
    "print(model.predict(bite_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('movinet_fine_tuned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
