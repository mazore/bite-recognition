{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "from extract_face import extract_face\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "from time import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "FRAME_RATE = 10\n",
    "SOURCE_FRAME_RATE = 30\n",
    "FRAME_STEP = (30 / FRAME_RATE)\n",
    "EPOCHS = 10\n",
    "FRAMES_PER_CLIP = 8\n",
    "\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 594\n",
      "Total videos for testing: 224\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "skipping frame 4365\n",
      "skipping frame 4368\n",
      "skipping frame 4371\n",
      "skipping frame 4374\n",
      "skipping frame 4377\n",
      "skipping frame 4380\n",
      "skipping frame 4383\n",
      "skipping frame 4386\n",
      "skipping frame 4389\n",
      "skipping frame 4431\n",
      "skipping frame 4434\n",
      "skipping frame 4437\n",
      "skipping frame 4440\n",
      "skipping frame 4443\n",
      "skipping frame 4446\n",
      "skipping frame 4449\n",
      "skipping frame 4452\n",
      "skipping frame 4455\n",
      "skipping frame 4458\n",
      "skipping frame 4461\n",
      "skipping frame 4464\n",
      "skipping frame 4467\n",
      "skipping frame 4470\n",
      "skipping frame 4473\n",
      "skipping frame 4476\n",
      "skipping frame 4479\n",
      "skipping frame 4482\n",
      "skipping frame 4485\n",
      "skipping frame 4488\n",
      "skipping frame 4491\n",
      "skipping frame 4494\n",
      "skipping frame 4497\n",
      "skipping frame 4500\n",
      "skipping frame 4503\n",
      "skipping frame 4506\n",
      "skipping frame 4509\n",
      "skipping frame 4512\n",
      "skipping frame 4515\n",
      "skipping frame 4518\n",
      "skipping frame 4521\n",
      "skipping frame 4524\n",
      "skipping frame 4527\n",
      "skipping frame 4530\n",
      "skipping frame 4533\n",
      "skipping frame 4536\n",
      "skipping frame 4539\n",
      "skipping frame 4542\n",
      "skipping frame 4545\n",
      "skipping frame 4548\n",
      "skipping frame 4551\n",
      "skipping frame 4554\n",
      "skipping frame 4557\n",
      "skipping frame 4560\n",
      "skipping frame 4563\n",
      "skipping frame 4566\n",
      "skipping frame 4569\n",
      "skipping frame 4572\n",
      "skipping frame 4575\n",
      "skipping frame 4578\n",
      "skipping frame 4581\n",
      "skipping frame 4584\n",
      "skipping frame 4587\n",
      "skipping frame 4590\n",
      "skipping frame 4593\n",
      "skipping frame 4596\n",
      "skipping frame 4599\n",
      "skipping frame 4602\n",
      "skipping frame 4605\n",
      "skipping frame 4608\n",
      "skipping frame 4611\n",
      "skipping frame 4614\n",
      "skipping frame 4617\n",
      "skipping frame 4620\n",
      "skipping frame 4623\n",
      "skipping frame 4626\n",
      "skipping frame 4629\n",
      "skipping frame 4632\n",
      "skipping frame 4635\n",
      "skipping frame 4638\n",
      "skipping frame 4641\n",
      "skipping frame 4644\n",
      "skipping frame 4647\n",
      "skipping frame 4650\n",
      "skipping frame 4653\n",
      "skipping frame 4656\n",
      "skipping frame 4659\n",
      "skipping frame 4662\n",
      "skipping frame 4665\n",
      "skipping frame 4668\n",
      "skipping frame 4671\n",
      "skipping frame 4674\n",
      "skipping frame 4677\n",
      "skipping frame 4680\n",
      "skipping frame 4683\n",
      "skipping frame 4686\n",
      "skipping frame 4689\n",
      "skipping frame 4692\n",
      "skipping frame 4695\n",
      "skipping frame 4698\n",
      "skipping frame 4701\n",
      "skipping frame 4704\n",
      "skipping frame 4707\n",
      "skipping frame 4710\n",
      "skipping frame 4713\n",
      "skipping frame 4716\n",
      "skipping frame 4719\n",
      "skipping frame 4722\n",
      "skipping frame 4725\n",
      "skipping frame 4728\n",
      "skipping frame 4731\n",
      "skipping frame 4734\n",
      "skipping frame 4737\n",
      "skipping frame 4740\n",
      "skipping frame 4743\n",
      "skipping frame 4746\n",
      "skipping frame 4749\n",
      "skipping frame 4752\n",
      "skipping frame 4755\n",
      "skipping frame 4758\n",
      "skipping frame 4761\n",
      "skipping frame 4764\n",
      "skipping frame 4767\n",
      "skipping frame 4770\n",
      "skipping frame 4773\n",
      "skipping frame 4776\n",
      "skipping frame 4779\n",
      "skipping frame 4782\n",
      "skipping frame 4785\n",
      "skipping frame 4788\n",
      "skipping frame 4791\n",
      "skipping frame 4794\n",
      "skipping frame 4797\n",
      "skipping frame 4800\n",
      "skipping frame 4803\n",
      "skipping frame 4806\n",
      "skipping frame 4809\n",
      "skipping frame 4812\n",
      "skipping frame 4815\n",
      "skipping frame 4818\n",
      "skipping frame 4821\n",
      "skipping frame 4824\n",
      "skipping frame 4827\n",
      "skipping frame 4830\n",
      "skipping frame 4833\n",
      "skipping frame 4836\n",
      "skipping frame 4839\n",
      "skipping frame 4842\n",
      "skipping frame 4845\n",
      "skipping frame 4848\n",
      "skipping frame 4851\n",
      "skipping frame 4854\n",
      "skipping frame 4857\n",
      "skipping frame 4860\n",
      "skipping frame 4863\n",
      "skipping frame 4866\n",
      "skipping frame 4869\n",
      "skipping frame 4872\n",
      "skipping frame 4875\n",
      "skipping frame 4878\n",
      "skipping frame 4881\n",
      "skipping frame 4884\n",
      "skipping frame 4887\n",
      "skipping frame 4890\n",
      "skipping frame 4893\n",
      "skipping frame 4896\n",
      "skipping frame 4899\n",
      "skipping frame 4902\n",
      "skipping frame 4905\n",
      "skipping frame 4908\n",
      "skipping frame 4911\n",
      "skipping frame 4914\n",
      "skipping frame 4917\n",
      "skipping frame 4920\n",
      "skipping frame 4923\n",
      "skipping frame 4926\n",
      "skipping frame 4929\n",
      "skipping frame 4932\n",
      "skipping frame 4935\n",
      "skipping frame 4938\n",
      "skipping frame 4941\n",
      "skipping frame 4944\n",
      "skipping frame 4947\n",
      "skipping frame 4950\n",
      "skipping frame 4953\n",
      "skipping frame 4956\n",
      "skipping frame 4959\n",
      "skipping frame 4962\n",
      "skipping frame 4965\n",
      "skipping frame 4968\n",
      "skipping frame 4971\n",
      "skipping frame 4974\n",
      "skipping frame 4977\n",
      "skipping frame 4980\n",
      "skipping frame 4983\n",
      "skipping frame 4986\n",
      "skipping frame 4989\n",
      "skipping frame 4992\n",
      "skipping frame 4995\n",
      "skipping frame 4998\n",
      "5000\n",
      "skipping frame 5001\n",
      "skipping frame 5004\n",
      "skipping frame 5007\n",
      "skipping frame 5010\n",
      "skipping frame 5013\n",
      "skipping frame 5016\n",
      "skipping frame 5019\n",
      "skipping frame 5022\n",
      "skipping frame 5025\n",
      "skipping frame 5028\n",
      "skipping frame 5031\n",
      "skipping frame 5034\n",
      "skipping frame 5037\n",
      "skipping frame 5040\n",
      "skipping frame 5043\n",
      "skipping frame 5046\n",
      "skipping frame 5049\n",
      "skipping frame 5052\n",
      "skipping frame 5055\n",
      "skipping frame 5058\n",
      "skipping frame 5061\n",
      "skipping frame 5064\n",
      "skipping frame 5067\n",
      "skipping frame 5070\n",
      "skipping frame 5073\n",
      "skipping frame 5076\n",
      "skipping frame 5079\n",
      "skipping frame 5946\n",
      "skipping frame 5949\n",
      "skipping frame 5952\n",
      "skipping frame 5955\n",
      "skipping frame 5958\n",
      "skipping frame 5961\n",
      "skipping frame 5964\n",
      "skipping frame 5967\n",
      "skipping frame 5970\n",
      "skipping frame 5973\n",
      "skipping frame 5976\n",
      "skipping frame 5979\n",
      "skipping frame 5982\n",
      "skipping frame 5985\n",
      "skipping frame 5988\n",
      "skipping frame 5991\n",
      "skipping frame 5994\n",
      "skipping frame 5997\n",
      "skipping frame 6000\n",
      "6000\n",
      "skipping frame 6003\n",
      "skipping frame 6006\n",
      "skipping frame 6009\n",
      "skipping frame 6012\n",
      "skipping frame 6015\n",
      "skipping frame 6018\n",
      "skipping frame 6021\n",
      "skipping frame 6024\n",
      "skipping frame 6027\n",
      "skipping frame 6030\n",
      "skipping frame 6033\n",
      "skipping frame 6036\n",
      "skipping frame 6039\n",
      "skipping frame 6042\n",
      "skipping frame 6045\n",
      "skipping frame 6048\n",
      "skipping frame 6051\n",
      "skipping frame 6054\n",
      "skipping frame 6057\n",
      "skipping frame 6060\n",
      "skipping frame 6063\n",
      "skipping frame 6066\n",
      "skipping frame 6069\n",
      "skipping frame 6072\n",
      "skipping frame 6075\n",
      "skipping frame 6078\n",
      "skipping frame 6081\n",
      "skipping frame 6084\n",
      "skipping frame 6087\n",
      "skipping frame 6090\n",
      "skipping frame 6093\n",
      "skipping frame 6096\n",
      "skipping frame 6099\n",
      "skipping frame 6102\n",
      "skipping frame 6105\n",
      "skipping frame 6108\n",
      "skipping frame 6111\n",
      "skipping frame 6114\n",
      "skipping frame 6117\n",
      "skipping frame 6120\n",
      "skipping frame 6123\n",
      "skipping frame 6126\n",
      "skipping frame 6129\n",
      "skipping frame 6132\n",
      "skipping frame 6138\n",
      "skipping frame 6141\n",
      "skipping frame 6144\n",
      "skipping frame 6147\n",
      "skipping frame 6150\n",
      "skipping frame 6153\n",
      "skipping frame 6156\n",
      "skipping frame 6159\n",
      "skipping frame 6162\n",
      "skipping frame 6165\n",
      "skipping frame 6168\n",
      "skipping frame 6171\n",
      "skipping frame 6174\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('bite_frame_indexes.json') as frame:\n",
    "    bite_frame_indexes = json.load(frame)\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y+min_dim, start_x : start_x+min_dim]\n",
    "\n",
    "def extract_all_frames():\n",
    "    video_capture = cv2.VideoCapture('../raw_session.mp4')\n",
    "    frames = {}\n",
    "    success, frame = video_capture.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        if count % FRAME_STEP == 0:  # Extract frames at the frame rate\n",
    "            # frame = crop_center_square(frame)\n",
    "            # Extract face\n",
    "            face = extract_face(frame)\n",
    "            if face is not None and face.size != 0:\n",
    "                face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "                face = face[:, :, [2, 1, 0]]\n",
    "                frames[count] = face\n",
    "            else:\n",
    "                print('skipping frame', count)\n",
    "                # cv2.imwrite(f\"test_frames/frame_{count}.jpg\", frame)\n",
    "        if count % 1000 == 0:\n",
    "            print(count)\n",
    "        success, frame = video_capture.read()\n",
    "        count += 1\n",
    "    video_capture.release()\n",
    "    return frames\n",
    "\n",
    "all_frames = extract_all_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame in enumerate(list(all_frames.values())[:10]):\n",
    "    cv2.imwrite(f\"test_frames/frame_{i}.jpg\", frame[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bites: 13\n",
      "Non bites: 278\n",
      "Train bites: 10\n",
      "Train non bites: 222\n",
      "Test bites: 3\n",
      "Test non bites: 56\n"
     ]
    }
   ],
   "source": [
    "bites = []\n",
    "non_bites = []\n",
    "current_clip = []\n",
    "for i, frame in all_frames.items():\n",
    "    if len(current_clip) < FRAMES_PER_CLIP:\n",
    "        current_clip.append(frame)  # Collect frames for the current clip\n",
    "    else:  # If the current clip is full, add it to the list of clips and start a new clip\n",
    "        first_index = i - (FRAMES_PER_CLIP * FRAME_STEP)\n",
    "        last_index = i - 1\n",
    "        is_bite = False\n",
    "        for bite_frame_index in bite_frame_indexes:\n",
    "            if first_index <= bite_frame_index <= last_index:\n",
    "                is_bite = True\n",
    "                break\n",
    "        if is_bite:\n",
    "            bites.append(np.array(current_clip))\n",
    "        else:\n",
    "            non_bites.append(np.array(current_clip))\n",
    "        current_clip = [frame]\n",
    "\n",
    "print('Bites:', len(bites))\n",
    "print('Non bites:', len(non_bites))\n",
    "\n",
    "train_bites = bites[:int(len(bites) * 0.8)]\n",
    "train_non_bites = non_bites[:int(len(non_bites) * 0.8)]\n",
    "test_bites = bites[int(len(bites) * 0.8):]\n",
    "test_non_bites = non_bites[int(len(non_bites) * 0.8):]\n",
    "\n",
    "print('Train bites:', len(train_bites))\n",
    "print('Train non bites:', len(train_non_bites))\n",
    "print('Test bites:', len(test_bites))\n",
    "print('Test non bites:', len(test_non_bites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two methods are taken from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(weights=\"imagenet\",\n",
    "                                                       include_top=False, pooling=\"avg\",\n",
    "                                                       input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(bites_list, non_bites_list):\n",
    "    num_samples = len(bites_list) + len(non_bites_list)\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    # frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(num_samples, FRAMES_PER_CLIP, NUM_FEATURES),\n",
    "                                dtype=\"float32\")\n",
    "    labels = np.zeros(shape=(num_samples), dtype=\"int\")\n",
    "\n",
    "    for is_bite, sequences in enumerate([non_bites_list, bites_list]):\n",
    "        for idx, frames in enumerate(sequences):\n",
    "            frames = frames[None, ...]\n",
    "\n",
    "            # Initialize placeholders to store the features of the current video.\n",
    "            temp_frame_features = np.zeros(shape=(1, FRAMES_PER_CLIP, NUM_FEATURES),\n",
    "                                    dtype=\"float32\")\n",
    "\n",
    "            # Extract features from the frames of the current video.\n",
    "            for i, frame in enumerate(frames):\n",
    "                for j in range(FRAMES_PER_CLIP):\n",
    "                    temp_frame_features[i, j, :] = feature_extractor.predict(frame[None, j, :])\n",
    "\n",
    "            print(is_bite, idx, '    ', temp_frame_features.shape)\n",
    "            frame_features[idx, ] = temp_frame_features.squeeze()\n",
    "            labels[idx] = is_bite\n",
    "\n",
    "    return frame_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (232, 8, 2048) and (59, 8, 2048)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_bites, train_non_bites)\n",
    "test_data, test_labels = prepare_all_videos(test_bites, test_non_bites)\n",
    "\n",
    "print(f\"Frame features in train set: {train_data.shape} and {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9511 - loss: 0.0801\n",
      "Epoch 1: val_loss improved from inf to 0.62868, saving model to ./model.keras\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - accuracy: 0.9493 - loss: 0.0817 - val_accuracy: 1.0000 - val_loss: 0.6287\n",
      "Epoch 2/10\n",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9688 - loss: 0.0249\n",
      "Epoch 2: val_loss did not improve from 0.62868\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9224 - loss: 0.0705 - val_accuracy: 0.8571 - val_loss: 0.6547\n",
      "Epoch 3/10\n",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9399 - loss: 0.0773\n",
      "Epoch 3: val_loss did not improve from 0.62868\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9377 - loss: 0.0776 - val_accuracy: 0.8571 - val_loss: 0.6441\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9058 - loss: 0.0782\n",
      "Epoch 4: val_loss improved from 0.62868 to 0.61919, saving model to ./model.keras\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9078 - loss: 0.0778 - val_accuracy: 0.8571 - val_loss: 0.6192\n",
      "Epoch 5/10\n",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9173 - loss: 0.0840\n",
      "Epoch 5: val_loss did not improve from 0.61919\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9209 - loss: 0.0823 - val_accuracy: 0.8571 - val_loss: 0.6234\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9036 - loss: 0.0692\n",
      "Epoch 6: val_loss did not improve from 0.61919\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9059 - loss: 0.0699 - val_accuracy: 0.8286 - val_loss: 0.6492\n",
      "Epoch 7/10\n",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9693 - loss: 0.0501\n",
      "Epoch 7: val_loss did not improve from 0.61919\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9604 - loss: 0.0575 - val_accuracy: 0.8143 - val_loss: 0.6297\n",
      "Epoch 8/10\n",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9375 - loss: 0.0748\n",
      "Epoch 8: val_loss improved from 0.61919 to 0.61640, saving model to ./model.keras\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9253 - loss: 0.0739 - val_accuracy: 0.8143 - val_loss: 0.6164\n",
      "Epoch 9/10\n",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9049 - loss: 0.0666\n",
      "Epoch 9: val_loss improved from 0.61640 to 0.60632, saving model to ./model.keras\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9085 - loss: 0.0701 - val_accuracy: 0.8000 - val_loss: 0.6063\n",
      "Epoch 10/10\n",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9114 - loss: 0.0728\n",
      "Epoch 10: val_loss did not improve from 0.60632\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9032 - loss: 0.0728 - val_accuracy: 0.7714 - val_loss: 0.6149\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9444 - loss: 0.5864\n",
      "Test accuracy: 93.22%\n"
     ]
    }
   ],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    frame_features_input = keras.Input((FRAMES_PER_CLIP, NUM_FEATURES))\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input)\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    rnn_model = keras.Model(frame_features_input, output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"./model.keras\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "        class_weight={0: 13 / 278, 1: 1},\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate(test_data, test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "history, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bites' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# clip = bites[2]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# features = np.zeros(shape=(1, FRAMES_PER_CLIP, NUM_FEATURES),\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#                                     dtype=\"float32\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# for j in range(FRAMES_PER_CLIP):\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     features[0, j, :] = feature_extractor.predict(clip[None, j, :])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# sequence_model.predict(features)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m sequence_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m features \u001b[38;5;241m=\u001b[39m feature_extractor\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mbites\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     11\u001b[0m sequence_model\u001b[38;5;241m.\u001b[39mpredict(features[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bites' is not defined"
     ]
    }
   ],
   "source": [
    "# clip = bites[2]\n",
    "# features = np.zeros(shape=(1, FRAMES_PER_CLIP, NUM_FEATURES),\n",
    "#                                     dtype=\"float32\")\n",
    "# for j in range(FRAMES_PER_CLIP):\n",
    "#     features[0, j, :] = feature_extractor.predict(clip[None, j, :])\n",
    "# sequence_model.predict(features)\n",
    "sequence_model = keras.models.load_model('./model.keras')\n",
    "\n",
    "\n",
    "features = feature_extractor.predict(bites[0])\n",
    "sequence_model.predict(features[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "[[0.49062577]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "[[0.50065696]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "[[0.52434826]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "[[0.52913594]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "[[0.52666116]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "[[0.516014]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "[[0.4804784]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "[[0.45276082]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[[0.45536885]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "[[0.44965732]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "[[0.5294156]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "[[0.5278702]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "[[0.51732385]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "[[0.51378024]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[[0.5274732]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "[[0.5270039]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[[0.52727437]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(current_clip)\n\u001b[1;32m---> 17\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m sequence_model\u001b[38;5;241m.\u001b[39mpredict(features[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:508\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    506\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m    507\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[1;32m--> 508\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[0;32m    510\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Evan Mazor\\code\\repos\\bite-recognition\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "success, frame = video_capture.read()\n",
    "count = 0\n",
    "current_clip = []\n",
    "while success:\n",
    "    # Extract face\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    face = extract_face(frame)\n",
    "    if face is not None and face.size != 0:\n",
    "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "        face = face[:, :, [2, 1, 0]]\n",
    "        if len(current_clip) < FRAMES_PER_CLIP:\n",
    "            current_clip.append(face)\n",
    "        else:\n",
    "            frames = np.array(current_clip)\n",
    "            features = feature_extractor.predict(frames)\n",
    "            prediction = sequence_model.predict(features[None, ...])\n",
    "            print(prediction)\n",
    "            current_clip = []\n",
    "\n",
    "    else:\n",
    "        print('skipping frame because no face')\n",
    "        # cv2.imwrite(f\"test_frames/frame_{count}.jpg\", frame)\n",
    "\n",
    "    cv2.waitKey(1000 // FRAME_RATE)\n",
    "\n",
    "    success, frame = video_capture.read()\n",
    "    count += 1\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
